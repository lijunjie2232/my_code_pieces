{
  "folders": [
    {
      "id": "FFoy_tpD",
      "name": "Default",
      "defaultLanguage": "typescript",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "createdAt": 1705547730793,
      "updatedAt": 1705547730793,
      "index": 0
    },
    {
      "name": "js",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "plain_text",
      "id": "8qmQazwt",
      "createdAt": 1705547881037,
      "updatedAt": 1705547887416,
      "index": 1
    },
    {
      "name": "nodejs",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "plain_text",
      "id": "OtjZFwbc",
      "createdAt": 1705559758412,
      "updatedAt": 1705559762435,
      "index": 2
    },
    {
      "name": "python",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "python",
      "id": "weWtbZI4",
      "createdAt": 1705649797466,
      "updatedAt": 1709526488611,
      "index": 3
    },
    {
      "name": "linux",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "sh",
      "id": "NCg2YrWJ",
      "createdAt": 1705655890816,
      "updatedAt": 1709526511683,
      "index": 4
    },
    {
      "name": "web",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "plain_text",
      "id": "fA481292",
      "createdAt": 1705739854728,
      "updatedAt": 1705739858907,
      "index": 5
    },
    {
      "name": "ts",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "typescript",
      "id": "RxpfS9zV",
      "createdAt": 1709687889308,
      "updatedAt": 1709687938343,
      "index": 6
    },
    {
      "name": "cmd",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "powershell",
      "id": "HRD2NCV8",
      "createdAt": 1711067461127,
      "updatedAt": 1711067487711,
      "index": 7
    },
    {
      "name": "algo",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "c_cpp",
      "id": "4ojt5eME",
      "createdAt": 1720877654533,
      "updatedAt": 1720877817895,
      "index": 8
    },
    {
      "name": "deeplearning",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "python",
      "id": "DvcfGvIx",
      "createdAt": 1724574331649,
      "updatedAt": 1724574577063,
      "index": 9
    },
    {
      "name": "java",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "java",
      "id": "PoO2O3Rm",
      "createdAt": 1729592825287,
      "updatedAt": 1729592889036
    },
    {
      "name": "android",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "kotlin",
      "id": "twJ-w0RU",
      "createdAt": 1729995664649,
      "updatedAt": 1729995682092
    },
    {
      "name": "sql",
      "parentId": null,
      "isOpen": false,
      "isSystem": false,
      "defaultLanguage": "mysql",
      "id": "mdpwqphi",
      "createdAt": 1730010819388,
      "updatedAt": 1730010903566
    }
  ],
  "snippets": [
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "8qmQazwt",
      "tagsIds": [],
      "description": null,
      "name": "xhr",
      "content": [
        {
          "label": "get",
          "language": "javascript",
          "value": "function loadDoc() {\n    // 第一步： 创建xhr对象\n    let xhr = new XMLHttpRequest();\n    // 第二步： 调用open函数 指定请求方式 与URL地址\n    xhr.open('GET', '/demo/music_list.xml', true);\n    // 第三步： 调用send函数 发起ajax请求\n    xhr.send();\n    // 第四步： 监听onreadystatechange事件\n    xhr.onreadystatechange = function () {\n        // 监听xhr对象的请求状态 与服务器的响应状态\n        if (this.readyState == 4 && this.status == 200) {\n            // 如果响应就绪的话，就创建表格(拿到了服务器响应回来的数据xhr.responseText)\n            myFunction(this)\n        }\n    }\n}\n\nxhr.open('GET', 'http://www.liulongbin.top:3006/api/getbooks?id=1')\n"
        },
        {
          "label": "post-formdata",
          "language": "javascript",
          "value": "// 第一步： 创建xhr对象\nlet xhr = new XMLHttpRequest();\n// 第二步： 调用open函数\nxhr.open('POST', 'http://www.liulongbin.top:3006/api/addbook')\n// 第三步： 设置Content-Type属性 （这一步是固定的写法）\nxhr.setRequestHeader('Conten-Type', 'application/x-www-form-urlencoded')\n// 第四步： 调用send（）函数，同时将数据以查询字符串的形式，提交给服务器\nxhr.send('bookname=水浒传&author=施耐庵&publisher=天津图书出版社')\n// 第五步：监听onreadystatechange事件\nxhr.onreadystatechange = function () {\n    if (xhr.readyState === 4 && xhr.status === 200) {\n        console.log(xhr.responseText)\n    }\n}"
        },
        {
          "label": "post-json",
          "language": "javascript",
          "value": "const xhr = new XMLHttpRequest()\n\n// listen for `load` event\nxhr.onload = () => {\n  // print JSON response\n  if (xhr.status >= 200 && xhr.status < 300) {\n    // parse JSON\n    const response = JSON.parse(xhr.responseText)\n    console.log(response)\n  }\n}\n\n// create a JSON object\nconst json = {\n  email: 'eve.holt@reqres.in',\n  password: 'cityslicka'\n}\n\n// open request\nxhr.open('POST', 'https://reqres.in/api/login')\n\n// set `Content-Type` header\nxhr.setRequestHeader('Content-Type', 'application/json')\n\n// send rquest with JSON payload\nxhr.send(JSON.stringify(json))"
        }
      ],
      "id": "jWmi9e8d",
      "createdAt": 1705547901298,
      "updatedAt": 1705548171733
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "8qmQazwt",
      "tagsIds": [],
      "description": null,
      "name": "ajax",
      "content": [
        {
          "label": "get",
          "language": "javascript",
          "value": "$.get('url', {name: 'zs', age: 20}, function() {})\n// 等价于\n$.get('url?name=zs&age=20', function() {})\n\n$.ajax({ method: 'GET', url: 'url', data: {name: 'zs', age: 20}, success: function() {} })\n// 等价于\n$.ajax({ method: 'GET', url: 'url?name=zs&age=20', success: function() {} })"
        },
        {
          "label": "json",
          "language": "plain_text",
          "value": "$.ajax({\n  url: 'example.com/api/data',\n  method: 'GET',\n  dataType: 'json',\n  success: function(data) {\n    // 成功接收到数据后的回调函数\n    // data 参数就是解析后的 JSON 数据\n    console.log(data); // 在控制台中打印数据\n\n    // 以下可以根据需要进行数据处理和展示\n    // 例如，将数据渲染到页面上：\n    var html = '';\n    $.each(data, function(index, item) {\n      html += '<li>' + item.name + '</li>';\n    });\n    $('#list').html(html);\n  },\n  error: function(xhr, status, error) {\n    // 处理请求错误的回调函数\n    console.log(error);\n  }\n});\n"
        }
      ],
      "id": "sAcJDFaN",
      "createdAt": 1705548024545,
      "updatedAt": 1705680788382
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "8qmQazwt",
      "tagsIds": [],
      "description": null,
      "name": "fetch api",
      "content": [
        {
          "label": "post-json",
          "language": "javascript",
          "value": "// create a JSON object\nconst json = {\n  email: 'hi@attacomsian.com',\n  password: '123abc'\n}\n\n// request options\nconst options = {\n  method: 'POST',\n  body: JSON.stringify(json),\n  headers: {\n    'Content-Type': 'application/json'\n  }\n}\n\n// send post request\nfetch('/login', options)\n  .then(res => res.json())\n  .then(res => console.log(res))\n  .catch(err => console.error(err))"
        },
        {
          "label": "get",
          "language": "javascript",
          "value": "fetch('/js/users.json')\n  .then(response => {\n    // handle the response data\n  })\n  .catch(error => {\n    // handle errors\n  })\n  \nfetch('https://reqres.in/api/users')\n  .then(response => response.json())\n  .then(data => {\n    data.data.forEach(user => {\n      console.log(`${user.id}: ${user.first_name} ${user.last_name}`)\n    })\n  })"
        },
        {
          "label": "header",
          "language": "javascript",
          "value": "// create an empty `Headers` object\nconst headers = new Headers()\n\n// add headers\nheaders.append('Content-Type', 'text/plain')\nheaders.append('Accept', 'application/json')\n\n// add custom headers\nheaders.append('X-AT-Platform', 'Desktop')\nheaders.append('X-AT-Source', 'Google Search')\n\n// check if the header exists\nheaders.has('Accept') // true\n\n// get headers\nheaders.get('Accept') // application/json\nheaders.get('X-AT-Source') // Google Search\n\n// update header value\nheaders.set('Content-Type', 'application/json')\n\n// remove headers\nheaders.delete('Content-Type')\nheaders.delete('X-AT-Platform')\n\n// Passing an object literal\nconst headers = new Headers({\n  'Content-Type': 'application/json',\n  Accept: 'application/json'\n})\n\n// OR\n\n// Passing an array of arrays\nconst headers = new Headers([\n  ['Content-Type', 'application/json'],\n  ['Accept', 'application/json']\n])\n\n// >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\nconst request = new Request('https://reqres.in/api/users', {\n  headers: headers\n})\n\nfetch(request)\n  .then(res => res.json())\n  .then(json => console.log(json))\n  .catch(err => console.error('Error:', err))"
        }
      ],
      "id": "klHkol_O",
      "createdAt": 1705548237079,
      "updatedAt": 1705681113608
    },
    {
      "isDeleted": true,
      "isFavorites": false,
      "folderId": "OtjZFwbc",
      "tagsIds": [],
      "description": null,
      "name": "Buffer",
      "content": [
        {
          "label": "子片段 1",
          "language": "javascript",
          "value": "let buf = Buffer.alloc(10);"
        }
      ],
      "id": "D1oJIFA5",
      "createdAt": 1705559765903,
      "updatedAt": 1705559826668
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "yaml",
      "content": [
        {
          "label": "read",
          "language": "python",
          "value": "import yaml\n\nwith open('voc.yaml', 'r', encoding='utf8') as f:\n\tdata = yaml.safe_load(f)"
        },
        {
          "label": "write",
          "language": "plain_text",
          "value": "import yaml\n\n# 创建配置字典\nconfig = {\n    'database': {\n        'host': 'localhost',\n        'port': 5432,\n        'name': 'mydb'\n    },\n    'app': {\n        'debug': True,\n        'log_level': 'info'\n    }\n}\n\n# 写入 YAML 文件\nwith open('config.yaml', 'w') as yaml_file:\n    yaml.dump(config, yaml_file)"
        }
      ],
      "id": "Gyt1iP24",
      "createdAt": 1705649805191,
      "updatedAt": 1705649964517
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "ruemal.yaml",
      "content": [
        {
          "label": "read",
          "language": "plain_text",
          "value": "from ruamel.yaml import YAML\nyaml=YAML(typ='safe')\nwith open('conf.yaml', 'r') as yaml_file:\n\tconf = yaml.load(yaml_file)"
        },
        {
          "label": "write",
          "language": "plain_text",
          "value": "import ruemal.yaml\n\n# 创建配置字典\nconfig = {\n    'database': {\n        'host': 'localhost',\n        'port': 5432,\n        'name': 'mydb'\n    },\n    'app': {\n        'debug': True,\n        'log_level': 'info'\n    }\n}\n\n# 写入 YAML 文件\nwith open('config.yaml', 'w') as yaml_file:\n    ruemal.yaml.dump(config, yaml_file)"
        }
      ],
      "id": "veB74x97",
      "createdAt": 1705649919184,
      "updatedAt": 1705827167283
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "argparse",
      "content": [
        {
          "label": "basic",
          "language": "plain_text",
          "value": "import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--verbose\", help=\"increase output verbosity\",\n                    action=\"store_true\")\nargs = parser.parse_args()"
        },
        {
          "label": "list",
          "language": "python",
          "value": "import argparse\n \nparser = argparse.ArgumentParser()\n \n# By default it will fail with multiple arguments.\nparser.add_argument('--default')\n \n# Telling the type to be a list will also fail for multiple arguments,\n# but give incorrect results for a single argument.\nparser.add_argument('--list-type', type=list)\n \n# This will allow you to provide multiple arguments, but you will get\n# a list of lists which is not desired.\nparser.add_argument('--list-type-nargs', type=list, nargs='+')\n \n# This is the correct way to handle accepting multiple arguments.\n# '+' == 1 or more.\n# '*' == 0 or more.\n# '?' == 0 or 1.\n# An int is an explicit number of arguments to accept.\nparser.add_argument('--nargs', nargs='+')\n \n# To make the input integers\nparser.add_argument('--nargs-int-type', nargs='+', type=int)\n \n# An alternate way to accept multiple inputs, but you must\n# provide the flag once per input. Of course, you can use\n# type=int here if you want.\nparser.add_argument('--append-action', action='append')\n \n# To show the results of the given option to screen.\nfor _, value in parser.parse_args()._get_kwargs():\n    if value is not None:\n        print(value)\n        \n        \n\"\"\"\n$ python arg.py --default 1234 2345 3456 4567\n...\narg.py: error: unrecognized arguments: 2345 3456 4567\n \n$ python arg.py --list-type 1234 2345 3456 4567\n...\narg.py: error: unrecognized arguments: 2345 3456 4567\n \n$ # Quotes won't help here... \n$ python arg.py --list-type \"1234 2345 3456 4567\"\n['1', '2', '3', '4', ' ', '2', '3', '4', '5', ' ', '3', '4', '5', '6', ' ', '4', '5', '6', '7']\n$ python arg.py --list-type-nargs 1234 2345 3456 4567\n[['1', '2', '3', '4'], ['2', '3', '4', '5'], ['3', '4', '5', '6'], ['4', '5', '6', '7']]\n$ python arg.py --nargs 1234 2345 3456 4567\n['1234', '2345', '3456', '4567']\n$ python arg.py --nargs-int-type 1234 2345 3456 4567\n[1234, 2345, 3456, 4567]\n$ # Negative numbers are handled perfectly fine out of the box.\n$ python arg.py --nargs-int-type -1234 2345 -3456 4567\n[-1234, 2345, -3456, 4567]\n$ python arg.py --append-action 1234 --append-action 2345 --append-action 3456 --append-action 4567\n\"\"\""
        }
      ],
      "id": "UP1uoVQH",
      "createdAt": 1705650796734,
      "updatedAt": 1724577088639
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "NCg2YrWJ",
      "tagsIds": [],
      "description": null,
      "name": "nvidia-smi",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "// 调整gpu持久性模式\nsudo nvidia-smi -pm 1\n// 调整gpu最大功率\nsudo nvidia-smi -pl 300"
        }
      ],
      "id": "26X8AOSD",
      "createdAt": 1705655898703,
      "updatedAt": 1705655972195
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "fA481292",
      "tagsIds": [],
      "description": null,
      "name": "http MIME",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "Multipurpose lnternet Mail Extensions:\n\nhtml: text/html\ncss: text/css\njs: text/javascript\npng: image/png\njpg: image/jpeg\ngif: image/gif\nmp4: video/mp4'\nmp3: audio/mpeg\njson: application/json"
        }
      ],
      "id": "HnMsiaZI",
      "createdAt": 1705739861641,
      "updatedAt": 1705740021013
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "jupyter",
      "content": [
        {
          "label": "reload",
          "language": "plain_text",
          "value": "%load_ext autoreload\n%autoreload 2"
        },
        {
          "label": "plot",
          "language": "python",
          "value": "%matplotlib inline"
        }
      ],
      "id": "Wry74Dzx",
      "createdAt": 1705886423955,
      "updatedAt": 1711760156929
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "Path",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "file = Path('/root/data/voc.yaml')\nfile.stem => 'voc'"
        }
      ],
      "id": "5e0YC450",
      "createdAt": 1706190040944,
      "updatedAt": 1706190107535
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "supression warning",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "import warnings\n\nwarnings.filterwarnings('ignore')\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)"
        }
      ],
      "id": "8qlwmnmh",
      "createdAt": 1706193575402,
      "updatedAt": 1715651037810
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "numpy",
      "content": [
        {
          "label": "dim+",
          "language": "plain_text",
          "value": "ann = np.expand_dims(ann,0)"
        },
        {
          "label": "min-max",
          "language": "plain_text",
          "value": "np.clip(a, 1, 8)"
        },
        {
          "label": "eu dist",
          "language": "python",
          "value": "def euclidean_distance(u, v):\n    \"\"\"\n    Returns the euclidean distance between vectors u and v. This is equivalent\n    to the length of the vector (u - v).\n    \"\"\"\n    diff = u - v\n    # return sqrt(numpy.dot(diff, diff.T))\n    return np.sum(np.diag(np.sqrt(np.dot(diff, diff.T))))"
        },
        {
          "label": "diag",
          "language": "plain_text",
          "value": "x=np.array([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\nnp.diag(x) => array([0, 4, 8])"
        }
      ],
      "id": "ZhL1nAet",
      "createdAt": 1706504886238,
      "updatedAt": 1706580864691
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "matplotlib",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# matplotlib折线图（标记点、标记点大小、标记点边颜色、标记点边宽\nfrom matplotlib import pyplot as plt\nx = range(1,10) #x轴的位置\ny = [6,7,12,12,15,17,15,20,18] #y轴的位置\n#传入x,y，通过plot画图,并设置折线颜色、透明度、折线样式和折线宽度  标记点、标记点大小、标记点边颜色、标记点边宽\nplt.plot(x,y,color='red',alpha=0.3,linestyle='--',linewidth=5,marker='o'\n         ,markeredgecolor='r',markersize='20',markeredgewidth=10)\nplt.show()\n"
        }
      ],
      "id": "ZKxV4l_b",
      "createdAt": 1706538922876,
      "updatedAt": 1706538967381
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "torch",
      "content": [
        {
          "label": "cuda cache",
          "language": "plain_text",
          "value": "torch.cuda.empty_cache()"
        },
        {
          "label": "ddp",
          "language": "python",
          "value": "import torch\nimport torch.nn.functional as F\nfrom utils import MyTrainDataset\n\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group, get_rank\nimport os\n\nLOCAL_RANK = int(\n    os.getenv(\"LOCAL_RANK\", -1)\n)  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv(\"RANK\", -1))\nWORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\n\ndef ddp_setup(rank: int, world_size: int):\n    \"\"\"\n    Args:\n        rank: Unique identifier of each process\n    world_size: Total number of processes\n    \"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\n    train_data = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=32,\n        shuffle=False,\n        sampler=DistributedSampler(train_dataset),\n    )\n\ndef _run_epoch(self, epoch):\n    b_sz = len(next(iter(self.train_data))[0])\n    self.train_data.sampler.set_epoch(epoch)\n    for source, targets in self.train_data:\n        ...\n        self._run_batch(source, targets)\n\ndef save_model(model, epoch, save_every):\n    def _save_checkpoint(ckp):\n        ...\n        pass\n\n    ckp = model.module.state_dict()\n    ...\n    ...\n    if get_rank() == 0 and epoch % save_every == 0:\n        _save_checkpoint(ckp)\n    \n    dist.barrier()\n    return\n\ndef main(rank, world_size, total_epochs, save_every):\n    ddp_setup(rank, world_size)\n    dataset, model, optimizer = load_train_objs()\n    train_data = prepare_dataloader(dataset, batch_size=32)\n    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n    trainer.train(total_epochs)\n    destroy_process_group()\n\nif __name__ == \"__main__\":\n    import sys\n    total_epochs = int(sys.argv[1])\n    save_every = int(sys.argv[2])\n    world_size = WORLD_SIZE\n    mp.spawn(main, args=(LOCAL_RANK, world_size, total_epochs, save_every,), nprocs=world_size)"
        }
      ],
      "id": "iUBzP4mv",
      "createdAt": 1706543184905,
      "updatedAt": 1725934447395
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "@thread",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "def threaded(func):\n    \"\"\"\n    Multi-threads a target function by default and returns the thread or function result.\n\n    Use as @threaded decorator. The function runs in a separate thread unless 'threaded=False' is passed.\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        \"\"\"Multi-threads a given function based on 'threaded' kwarg and returns the thread or function result.\"\"\"\n        if kwargs.pop(\"threaded\", True):  # run in thread\n            thread = threading.Thread(target=func, args=args, kwargs=kwargs, daemon=True)\n            thread.start()\n            return thread\n        else:\n            return func(*args, **kwargs)\n\n    return wrapper"
        }
      ],
      "id": "CBAeSmOT",
      "createdAt": 1706544304341,
      "updatedAt": 1706544314752
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "multiprocessing",
      "content": [
        {
          "label": "apply_async",
          "language": "plain_text",
          "value": "#apply  (阻塞，同步方式)\nfrom  multiprocessing import Pool\nimport time\n \n#apply_async   (非阻塞，异步方式)\ndef f1(i):\n    time.sleep(0.5)\n    print(i)\n    return i + 100\ndef f2(arg):\n    print(arg)\n \nif __name__ == \"__main__\":\n    pool = Pool(5)\n    for i in range(1,31):\n        pool.apply_async(func=f1,args=(i,),callback=f2)\n    pool.close()\n    pool.join()\n"
        },
        {
          "label": "cpu_count",
          "language": "plain_text",
          "value": "from multiprocessing import cpu_count\n\nprint(\"CPU的核数为：{}\".format(cpu_count()))\nprint(type(cpu_count()))"
        },
        {
          "label": "submit",
          "language": "plain_text",
          "value": "from concurrent.futures import ProcessPoolExecutor\nimport  time\ndef task(name):\n    print(\"name\",name)\n    time.sleep(1)\n\nif __name__ == \"__main__\":\n    start = time.time()\n    ex = ProcessPoolExecutor(2)\n\n    for i in range(5):\n        ex.submit(task,\"safly%d\"%i)\n    ex.shutdown(wait=True)\n\n    print(\"main\")\n    end = time.time()\n    print(end - start)"
        },
        {
          "label": "map",
          "language": "plain_text",
          "value": "from multiprocessing import Pool\n\ndef f(x):\n    return x*x\n\nif __name__ == '__main__':\n    with Pool(5) as p:\n        print(p.map(f, [1, 2, 3]))"
        },
        {
          "label": "apply",
          "language": "plain_text",
          "value": "#apply  (阻塞，同步方式)\nfrom  multiprocessing import Pool\nimport time\n \ndef f1(i):\n    time.sleep(0.5)\n    print(i)\n    return i + 100\n \nif __name__ == \"__main__\":\n    pool = Pool(5)\n    for i in range(1,31):\n        pool.apply(func=f1,args=(i,))"
        },
        {
          "label": "imap",
          "language": "python",
          "value": "from tqdm import tqdm    \nfrom torch.multiprocessing import Pool\n\nwith Pool(15) as pool:\n    results = pool.imap(func=get_image_sm, iterable=zip(repeat(path), image_list))\n    pbar = tqdm(results, desc=\"calc\", total=len(image_list))\n    for result in pbar:\n        if isinstance(result, tuple):\n            img_count += 1\n            mean += result[0]\n            std += result[1]\n            pbar.set_postfix({\"mean\": mean / img_count, \"std\": std / img_count})\n        else:\n            print(result)\n    pbar.close()"
        }
      ],
      "id": "CUsUEUiX",
      "createdAt": 1706577534588,
      "updatedAt": 1724908222026
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "concurrent",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, wait, ALL_COMPLETED\nimport time, random, os\n\ndef piao(name, n):\n    print('%s is piaoing %s' % (name, os.getpid()))\n    time.sleep(1)\n    return n ** 2\n\n\nif __name__ == '__main__':\n\t\t########################################\n    p = ProcessPoolExecutor(2)\n    objs = []\n    start = time.time()\n    for i in range(5):\n        obj = p.submit(piao, 'safly %s' % i, i)  # 异步调用\n        objs.append(obj)\n\n    p.shutdown(wait=True)\n    print('主', os.getpid())\n    for obj in objs:\n        print(obj.done())\n        print(obj.result(timeout=None))\n\n    stop = time.time()\n    print(stop - start)\n\t\t########################################\n    p = ThreadPoolExecutor(2)\n    objs = []\n    start = time.time()\n    for i in range(5):\n        obj = p.submit(piao, '%s' % i, i)  # 异步调用\n        objs.append(obj)\n\n    print('主', os.getpid())\n    for obj in as_completed(objs):\n        print(obj.result())\n\t\t########################################\n    p = ThreadPoolExecutor(2)\n    objs = []\n    start = time.time()\n    for i in range(5):\n        obj = p.submit(piao, '%s' % i, i)  # 异步调用\n        objs.append(obj)\n\n    print('主', os.getpid())\n    wait(objs, return_when=ALL_COMPLETED)\n    for obj in objs:\n        print(obj.done())\n        print(obj.result(timeout=None))\n\n    stop = time.time()\n    print(stop - start)\n    p.shutdown(wait=True)"
        }
      ],
      "id": "v00hMsQs",
      "createdAt": 1706578242881,
      "updatedAt": 1727002786295
    },
    {
      "isDeleted": true,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "未命名程式碼片段",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": ""
        }
      ],
      "id": "RYf5I8fv",
      "createdAt": 1706580332307,
      "updatedAt": 1706580336957
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "OtjZFwbc",
      "tagsIds": [],
      "description": null,
      "name": "guard process",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "var fork = require('child_process').fork;\n\n//保存被子进程实例数组\nvar workers = [];\n\n//这里的被子进程理论上可以无限多\nvar appsPath = ['./app.js'];\n\nvar createWorker = function(appPath){\n　　//保存fork返回的进程实例\n　　var worker = fork(appPath);\n\n　　//监听子进程exit事件\n　　worker.on('exit',function(){\n　　　　console.log('worker:' + worker.pid + 'exited');\n　　　　delete workers[worker.pid];\n　　　　createWorker(appPath);\n　　 });\n\n　　workers[worker.pid] = worker;\n　　console.log('Create worker:' + worker.pid);\n};\n\n//启动所有子进程\nfor (var i = appsPath.length - 1; i >= 0; i--) {\n　　createWorker(appsPath[i]);\n}\n\n//父进程退出时杀死所有子进程\nprocess.on('exit',function(){\n　　 for(var pid in workers){\n　　　　workers[pid].kill();\n　　}\n});"
        }
      ],
      "id": "24sDH8Hh",
      "createdAt": 1706666147942,
      "updatedAt": 1706666155658
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "pickle",
      "content": [
        {
          "label": "dump",
          "language": "plain_text",
          "value": "with open('data.pickle', 'wb') as f:\n    pickle.dump(data, f)"
        },
        {
          "label": "dumps",
          "language": "plain_text",
          "value": "import pickle\ndic = {\"k1\":\"v1\",\"k2\":123}\ns = pickle.dumps(dic)\nprint(s)"
        },
        {
          "label": "load",
          "language": "plain_text",
          "value": "with open('data.pickle', 'rb') as f:\n    data = pickle.load(f)"
        },
        {
          "label": "loads",
          "language": "plain_text",
          "value": "import pickle\ndic = {\"k1\":\"v1\",\"k2\":123}\ns = pickle.dumps(dic)\ndic2 = pickle.loads(s)\nprint(dic2)"
        }
      ],
      "id": "gw3WAZqO",
      "createdAt": 1708936622268,
      "updatedAt": 1708936691956
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "progress",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# coding=utf-8\nfrom progress.bar import Bar\nimport time\n\n# 创建Bar类的实例\nbar = Bar('MyProcess:', max=100)\n# 循环处理某业务，调用bar对象的next()方法，循环次数等于max\nfor _ in range(100):\n # Do some work\n    time.sleep(0.05)\n    bar.next()\n# 循环完成后调用finish()方法\nbar.finish()\n\n\nwith Bar('Processing', max=20) as bar:\n    for i in range(20):\n        time.sleep(0.05)\n        bar.next()\n        \nfor i in Bar(\n    \t\tf\"{image_set}{year}\", bar_prefix=\" [\", bar_suffix=\"] \", fill=\">\"\n    ).iter(range(100)):\n    time.sleep(0.05)"
        }
      ],
      "id": "54MBrGJN",
      "createdAt": 1709098599140,
      "updatedAt": 1709989377044
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "get_VOC_ds",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import xml.etree.ElementTree as ET\n\nfrom tqdm import tqdm\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\nimport yaml as yl\nimport numpy as np\nimport copy\nfrom progress.bar import Bar\nimport os\nimport subprocess\n\nwith open(\"voc.yaml\", \"r\", encoding=\"utf8\") as f:\n    yaml = yl.safe_load(f)\n\n\ndef convert_label(path, lb_path, year, image_id, tho=0, class_filter=None):\n    def convert_box(size, box):\n        dw, dh = 1.0 / size[0], 1.0 / size[1]\n        x, y, w, h = (\n            (box[0] + box[1]) / 2.0 - 1,\n            (box[2] + box[3]) / 2.0 - 1,\n            box[1] - box[0],\n            box[3] - box[2],\n        )\n        return x * dw, y * dh, w * dw, h * dh\n\n    in_file = open(path / f\"VOC{year}/Annotations/{image_id}.xml\")\n    out_file = open(lb_path, \"w\")\n    tree = ET.parse(in_file)\n    root = tree.getroot()\n    size = root.find(\"size\")\n    w = int(size.find(\"width\").text)\n    h = int(size.find(\"height\").text)\n\n    names = list(yaml[\"names\"].values())  # names list\n    for obj in root.iter(\"object\"):\n        cls = obj.find(\"name\").text\n        if cls in names and int(obj.find(\"difficult\").text) != 1:\n            xmlbox = obj.find(\"bndbox\")\n            bb = convert_box(\n                (w, h),\n                [float(xmlbox.find(x).text) for x in (\"xmin\", \"xmax\", \"ymin\", \"ymax\")],\n            )\n            cls_id = names.index(cls)  # class id\n            if class_filter:\n                if class_filter(cls_id):\n                    out_file.write(\" \".join(str(a) for a in (cls_id, *bb)) + \"\\n\")\n            else:\n                if np.random.random() >= tho:\n                    out_file.write(\" \".join(str(a) for a in (cls_id, *bb)) + \"\\n\")\n                else:\n                    out_file.write(\" \".join(str(a) for a in (0, *bb)) + \"\\n\")\n\n\n# Download\ndir = Path(yaml[\"path\"])  # dataset root dir\n# url = 'https://mirror.ghproxy.com/github.com/ultralytics/yolov5/releases/download/v1.0/'\nurl = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/\"\nurls = [\n    f\"{url}VOCtrainval_06-Nov-2007.zip\",  # 446MB, 5012 images\n    f\"{url}VOCtest_06-Nov-2007.zip\",  # 438MB, 4953 images\n    f\"{url}VOCtrainval_11-May-2012.zip\",\n]  # 1.95GB, 17126 images\n# download(\n#     urls,\n#     dir=dir / \"images\",\n#     curl=True,\n#     threads=1,\n#     exist_ok=True,\n# )  # download and unzip over existing paths (required)\n\n# Convert origin\ndir = Path(yaml[\"path\"]) / \"origin\"\nos.makedirs(dir, exist_ok=True)\nsubprocess.run(\n    [\n        \"rsync\",\n        \"-auvrt\",\n        str((Path(yaml[\"path\"]) / \"images\").absolute()),\n        str(dir.absolute()),\n    ]\n)\npath = dir / \"images/VOCdevkit\"\nfor year, image_set in (\n    (\"2012\", \"train\"),\n    (\"2012\", \"val\"),\n    (\"2007\", \"train\"),\n    (\"2007\", \"val\"),\n    (\"2007\", \"test\"),\n):\n    imgs_path = dir / \"images\" / f\"{image_set}{year}\"\n    lbs_path = dir / \"labels\" / f\"{image_set}{year}\"\n    imgs_path.mkdir(exist_ok=True, parents=True)\n    lbs_path.mkdir(exist_ok=True, parents=True)\n\n    with open(path / f\"VOC{year}/ImageSets/Main/{image_set}.txt\") as f:\n        image_ids = f.read().strip().split()\n    for id in Bar(\n        f\"{image_set}{year}\", bar_prefix=\" [\", bar_suffix=\"] \", fill=\">\"\n    ).iter(image_ids):\n        f = path / f\"VOC{year}/JPEGImages/{id}.jpg\"  # old img path\n        lb_path = (lbs_path / f.name).with_suffix(\".txt\")  # new label path\n        f.rename(imgs_path / f.name)  # move image\n        convert_label(path, lb_path, year, id)  # convert labels to YOLO format\n    new_yaml = copy.deepcopy(yaml)\n    new_yaml[\"path\"] = dir.absolute()\n    with open(str(dir / \"data.yaml\"), \"w\") as yaml_file:\n        yl.dump(new_yaml, yaml_file)\n\n# Convert zen obj\ndir = Path(yaml[\"path\"]) / \"obj\"  # dataset root dir\nos.makedirs(dir, exist_ok=True)\nsubprocess.run(\n    [\n        \"rsync\",\n        \"-auvrt\",\n        str((Path(yaml[\"path\"]) / \"images\").absolute()),\n        str(dir.absolute()),\n    ]\n)\npath = dir / \"images/VOCdevkit\"\nfor year, image_set in (\n    (\"2012\", \"train\"),\n    (\"2012\", \"val\"),\n    (\"2007\", \"train\"),\n    (\"2007\", \"val\"),\n    (\"2007\", \"test\"),\n):\n    imgs_path = dir / \"images\" / f\"{image_set}{year}\"\n    lbs_path = dir / \"labels\" / f\"{image_set}{year}\"\n    imgs_path.mkdir(exist_ok=True, parents=True)\n    lbs_path.mkdir(exist_ok=True, parents=True)\n\n    with open(path / f\"VOC{year}/ImageSets/Main/{image_set}.txt\") as f:\n        image_ids = f.read().strip().split()\n    for id in Bar(\n        f\"{image_set}{year}\", bar_prefix=\" [\", bar_suffix=\"] \", fill=\">\"\n    ).iter(image_ids):\n        f = path / f\"VOC{year}/JPEGImages/{id}.jpg\"  # old img path\n        lb_path = (lbs_path / f.name).with_suffix(\".txt\")  # new label path\n        f.rename(imgs_path / f.name)  # move image\n        convert_label(path, lb_path, year, id, tho=1)  # convert labels to YOLO format\n    new_yaml = copy.deepcopy(yaml)\n    new_yaml[\"path\"] = dir.absolute()\n    new_yaml[\"names\"] = {0: \"unknown object\"}\n    for i in range(len(yaml[\"names\"])):\n        new_yaml[i + 1] = yaml[\"names\"][i]\n    with open(str(dir / \"data.yaml\"), \"w\") as yaml_file:\n        yl.dump(new_yaml, yaml_file)\n\n# Convert han obj\ndir = Path(yaml[\"path\"]) / \"obj_50pa\"  # dataset root dir\nos.makedirs(dir, exist_ok=True)\nsubprocess.run(\n    [\n        \"rsync\",\n        \"-auvrt\",\n        str((Path(yaml[\"path\"]) / \"images\").absolute()),\n        str(dir.absolute()),\n    ]\n)\npath = dir / \"images/VOCdevkit\"\nfor year, image_set in (\n    (\"2012\", \"train\"),\n    (\"2012\", \"val\"),\n    (\"2007\", \"train\"),\n    (\"2007\", \"val\"),\n    (\"2007\", \"test\"),\n):\n    imgs_path = dir / \"images\" / f\"{image_set}{year}\"\n    lbs_path = dir / \"labels\" / f\"{image_set}{year}\"\n    imgs_path.mkdir(exist_ok=True, parents=True)\n    lbs_path.mkdir(exist_ok=True, parents=True)\n\n    with open(path / f\"VOC{year}/ImageSets/Main/{image_set}.txt\") as f:\n        image_ids = f.read().strip().split()\n    for id in Bar(\n        f\"{image_set}{year}\", bar_prefix=\" [\", bar_suffix=\"] \", fill=\">\"\n    ).iter(image_ids):\n        f = path / f\"VOC{year}/JPEGImages/{id}.jpg\"  # old img path\n        lb_path = (lbs_path / f.name).with_suffix(\".txt\")  # new label path\n        f.rename(imgs_path / f.name)  # move image\n        convert_label(path, lb_path, year, id, tho=0.5)  # convert labels to YOLO format\n    new_yaml = copy.deepcopy(yaml)\n    new_yaml[\"path\"] = dir.absolute()\n    new_yaml[\"names\"] = {0: \"unknown object\"}\n    for i in range(len(yaml[\"names\"])):\n        new_yaml[i + 1] = yaml[\"names\"][i]\n    with open(str(dir / \"data.yaml\"), \"w\") as yaml_file:\n        yl.dump(new_yaml, yaml_file)\n\n# Convert continous learning dataset\nclass_stage = [[0, 6], [6, 13], [13, 20]]\nfor i, j in class_stage:\n    class_filter_testval = lambda x: x < j\n    class_filter_train = lambda x: x >= i and x < j\n    dir = Path(yaml[\"path\"]) / f\"split_{i}_{j}\"  # dataset root dir\n    os.makedirs(dir, exist_ok=True)\n    subprocess.run(\n        [\n            \"rsync\",\n            \"-auvrt\",\n            str((Path(yaml[\"path\"]) / \"images\").absolute()),\n            str(dir.absolute()),\n        ]\n    )\n    path = dir / \"images/VOCdevkit\"\n    for year, image_set in (\n        (\"2012\", \"train\"),\n        (\"2012\", \"val\"),\n        (\"2007\", \"train\"),\n        (\"2007\", \"val\"),\n        (\"2007\", \"test\"),\n    ):\n        imgs_path = dir / \"images\" / f\"{image_set}{year}\"\n        lbs_path = dir / \"labels\" / f\"{image_set}{year}\"\n        imgs_path.mkdir(exist_ok=True, parents=True)\n        lbs_path.mkdir(exist_ok=True, parents=True)\n\n        with open(path / f\"VOC{year}/ImageSets/Main/{image_set}.txt\") as f:\n            image_ids = f.read().strip().split()\n\n        for id in Bar(\n            f\"{image_set}{year}\", bar_prefix=\" [\", bar_suffix=\"] \", fill=\">\"\n        ).iter(image_ids):\n            f = path / f\"VOC{year}/JPEGImages/{id}.jpg\"  # old img path\n            lb_path = (lbs_path / f.name).with_suffix(\".txt\")  # new label path\n            f.rename(imgs_path / f.name)  # move image\n            convert_label(\n                path,\n                lb_path,\n                year,\n                id,\n                class_filter=(\n                    class_filter_testval if image_set == \"test\" else class_filter_train\n                ),\n            )  # convert labels to YOLO format\n        new_yaml = copy.deepcopy(yaml)\n        new_yaml[\"path\"] = dir.absolute()\n        new_yaml[\"names\"] = {}\n        for ii in range(j):\n            new_yaml[ii] = yaml[\"names\"][ii]\n        with open(str(dir / \"data.yaml\"), \"w\") as yaml_file:\n            yl.dump(new_yaml, yaml_file)\n"
        }
      ],
      "id": "SRdwFBY0",
      "createdAt": 1709358035263,
      "updatedAt": 1724662697548
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "NCg2YrWJ",
      "tagsIds": [],
      "description": null,
      "name": "shell echo font",
      "content": [
        {
          "label": "color",
          "language": "plain_text",
          "value": "字体颜色\n字体颜色：30-37\n\n默认=0\n黑色=30\n红色=31\n绿色=32\n黄色=33\n蓝色=34\n紫色=35\n天蓝色（青色）=36\n白色=37\n# echo -e \"\\e[30m 黑色 \\e[0m\"\n# echo -e \"\\e[31m 红色 \\e[0m\"\n# echo -e \"\\e[32m 绿色 \\e[0m\"\n# echo -e \"\\e[33m 黄色 \\e[0m\"\n# echo -e \"\\e[34m 蓝色 \\e[0m\"\n# echo -e \"\\e[35m 紫色 \\e[0m\"\n# echo -e \"\\e[36m 青色 \\e[0m\"\n# echo -e \"\\e[37m 白色 \\e[0m\"\n\n背景颜色\n背景颜色：40-47\n\n默认=0\n黑色=40\n红色=41\n绿色=42\n黄色=43\n蓝色=44\n紫色=45\n天蓝色（青色）=46\n白色=47\n# echo -e \"\\e[40m 黑底 \\e[0m\"\n# echo -e \"\\e[41m 红底 \\e[0m\"\n# echo -e \"\\e[42m 绿底 \\e[0m\"\n# echo -e \"\\e[43m 黄底 \\e[0m\"\n# echo -e \"\\e[44m 蓝底 \\e[0m\"\n# echo -e \"\\e[45m 紫底 \\e[0m\"\n# echo -e \"\\e[46m 青底 \\e[0m\"\n# echo -e \"\\e[47m 白底 \\e[0m\"\n\n黑底彩色\n黑底彩色：90-97\n\n黑=90\n深红=91\n绿=92\n黄色=93\n蓝色=94\n紫色=95\n深绿（青色）=96\n白色=97\n# echo -e \"\\e[90m 黑底黑字 \\e[0m\"\n# echo -e \"\\e[91m 黑底红字 \\e[0m\"\n# echo -e \"\\e[92m 黑底绿字 \\e[0m\"\n# echo -e \"\\e[93m 黑底黄字 \\e[0m\"\n# echo -e \"\\e[94m 黑底蓝字 \\e[0m\"\n# echo -e \"\\e[95m 黑底紫字 \\e[0m\"\n# echo -e \"\\e[96m 黑底青字 \\e[0m\"\n# echo -e \"\\e[97m 黑底白字 \\e[0m\"\n\n## 字体控制选项\n\\e[0m 关闭所有属性\n\\e[1m 设置高亮度\n\\e[4m 下划线\n\\e[5m 闪烁\n\\e[7m 反显，撞色显示，显示为白字黑底，或者显示为黑底白字\n\\e[8m 消影，字符颜色将会与背景颜色相同\n\\e[nA 光标上移 n 行\n\\e[nB 光标下移 n 行\n\\e[nC 光标右移 n 行\n\\e[nD 光标左移 n 行\n\\e[y;xH 设置光标位置\n\\e[2J 清屏\n\\e[K 清除从光标到行尾的内容\n\\e[s 保存光标位置\n\\e[u 恢复光标位置\n\\e[?25 隐藏光标\n\\e[?25h 显示光标"
        },
        {
          "label": "子片段 2",
          "language": "plain_text",
          "value": ""
        }
      ],
      "id": "blnb3Iy1",
      "createdAt": 1709523344717,
      "updatedAt": 1709523454756
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "NCg2YrWJ",
      "tagsIds": [],
      "description": null,
      "name": "tput",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "tput Color Capabilities:\n\ntput setab [0-7] – Set a background color using ANSI escape\ntput setb [0-7] – Set a background color\ntput setaf [0-7] – Set a foreground color using ANSI escape\ntput setf [0-7] – Set a foreground color\n\nColor Code for tput:\n\n0 – Black\n1 – Red\n2 – Green\n3 – Yellow\n4 – Blue\n5 – Magenta\n6 – Cyan\n7 – White\n\ntput Text Mode Capabilities:\n\ntput bold – Set bold mode\ntput dim – turn on half-bright mode\ntput smul – begin underline mode\ntput rmul – exit underline mode\ntput rev – Turn on reverse mode\ntput smso – Enter standout mode (bold on rxvt)\ntput rmso – Exit standout mode\ntput sgr0 – Turn off all attributes"
        },
        {
          "label": "子片段 2",
          "language": "sh",
          "value": "#!/bin/bash\n\nprintf $(tput setaf 2; tput bold)'color show\\n\\n'$(tput sgr0)\n\nfor((i=0; i<=7; i++)); do\n\techo $(tput setaf $i)\"show me the money\"$(tput sgr0)\ndone\n\nprintf '\\n'$(tput setaf 2; tput setab 0; tput bold)'background color show'$(tput sgr0)'\\n\\n'\n\nfor((i=0,j=7; i<=7; i++,j--)); do\n\techo $(tput setaf $i; tput setab $j; tput bold)\"show me the money\"$(tput sgr0)\ndone\n\nexit 0\n"
        }
      ],
      "id": "rl2tSchb",
      "createdAt": 1709523474208,
      "updatedAt": 1709526826833
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "RxpfS9zV",
      "tagsIds": [],
      "description": null,
      "name": "遍历数组",
      "content": [
        {
          "label": "子片段 1",
          "language": "typescript",
          "value": "/*TypeScript继承自JavaScript，因此可以使用JavaScript中的所有数组遍历方法，包括：\n\nfor循环\n可以使用传统的for循环遍历数组。例如：*/\n\nconst arr: number[] = [1, 2, 3, 4, 5];\n\nfor (let i = 0; i < arr.length; i++) {\n  console.log(arr[i]);\n}\n/*forEach()方法\n可以使用forEach()方法遍历数组，它接受一个回调函数作为参数，回调函数接受三个参数：当前元素的值、当前元素的索引和数组本身。例如：*/\n\nconst arr: number[] = [1, 2, 3, 4, 5];\n\narr.forEach((value, index, array) => {\n  console.log(value);\n});\n/*map()方法\n可以使用map()方法遍历数组，它接受一个回调函数作为参数，回调函数返回一个新的数组，新数组的元素是根据原数组的元素经过处理后得到的。例如：*/\n\nconst arr: number[] = [1, 2, 3, 4, 5];\n\nconst newArr = arr.map((value, index, array) => {\n  return value * 2;\n});\n\nconsole.log(newArr);\n/*filter()方法\n可以使用filter()方法遍历数组，它接受一个回调函数作为参数，回调函数返回一个布尔值，表示当前元素是否应该被包含在新的数组中。例如：*/\n\nconst arr: number[] = [1, 2, 3, 4, 5];\n\nconst filteredArr = arr.filter((value, index, array) => {\n  return value % 2 === 0;\n});\n\nconsole.log(filteredArr);\n/*reduce()方法\n可以使用reduce()方法遍历数组，它接受一个回调函数作为参数，回调函数接受四个参数：累加器、当前元素的值、当前元素的索引和数组本身。回调函数返回的值作为下一次调用回调函数的累加器的值。例如：*/\n\nconst arr: number[] = [1, 2, 3, 4, 5];\n\nconst sum = arr.reduce((accumulator, currentValue, currentIndex, array) => {\n  return accumulator + currentValue;\n}, 0);\n\nconsole.log(sum);\n/*还有其他一些数组遍历方法，如some()、every()、find()、findIndex()等，它们的使用方法与上述方法类似，根据实际需求选择适合的方法即可。*/"
        }
      ],
      "id": "_qQRFf4S",
      "createdAt": 1709687898236,
      "updatedAt": 1709687973606
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "lxml",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "from lxml import etree\n\nroot=etree.Element('root')\nprint(root.tag)\nchild=etree.SubElement(root,'child') # 添加一个子节点\nchild.set('id','test_Id')\nprint(etree.tostring(root))          # tostring 为序列化"
        },
        {
          "label": "xpath",
          "language": "python",
          "value": "from lxml import etree\n\nhtml = etree.parse('./test.html', etree.HTMLParser())\nresult = etree.tostring(html)\nprint(result.decode('utf-8'))"
        }
      ],
      "id": "sVMMZEu2",
      "createdAt": 1709787008159,
      "updatedAt": 1710954937124
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "snowflake",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# pip install snowflake-id\nfrom snowflake import SnowflakeGenerator\n\ngen = SnowflakeGenerator(42)\n\nfor i in range(100):\n    val = next(gen)\n    print(val)"
        }
      ],
      "id": "izjU0ZO0",
      "createdAt": 1710000893638,
      "updatedAt": 1710000948908
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "Process",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# 导入进程模块\nimport multiprocessing\n \n# 最多允许3个进程同时运行\npool = multiprocessing.Pool(processes = 3)\n \n1、apply() — 该函数用于传递不定参数，主进程会被阻塞直到函数执行结束（不建议使用，并且3.x以后不在出现），函数原型如下：\n\napply(func, args=(), kwds={})\n2、apply_async — 与apply用法一致，但它是非阻塞的且支持结果返回后进行回调，函数原型如下：\n\napply_async(func[, args=()[, kwds={}[, callback=None]]])\n3、map() — Pool类中的map方法，与内置的map函数用法基本一致，它会使进程阻塞直到结果返回，函数原型如下：\n\nmap(func, iterable, chunksize=None)\n注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。\n\n4、map_async() — 与map用法一致，但是它是非阻塞的。其有关事项见apply_async，函数原型如下：\n\nmap_async(func, iterable, chunksize, callback)\n5、close() — 关闭进程池（pool），使其不在接受新的任务。\n\n6、terminal() — 结束工作进程，不在处理未处理的任务。\n\n7、join() — 主进程阻塞等待子进程的退出， join方法要在close或terminate之后使用。"
        }
      ],
      "id": "lapLuKwb",
      "createdAt": 1710003507218,
      "updatedAt": 1710003515804
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "sqlite3",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import sqlite3\ncon = sqlite3.connect(\"tutorial.db\")\n\ncur = con.cursor()\ncur.execute(\"CREATE TABLE movie(title, year, score)\")\nres = cur.execute(\"SELECT name FROM sqlite_master\")\nres.fetchone()"
        }
      ],
      "id": "jftvFjm0",
      "createdAt": 1710206891856,
      "updatedAt": 1710206929164
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "HRD2NCV8",
      "tagsIds": [],
      "description": null,
      "name": "gpupdate /force",
      "content": [
        {
          "label": "子片段 1",
          "language": "powershell",
          "value": "gpupdate /force"
        }
      ],
      "id": "59rdBTM8",
      "createdAt": 1711067467424,
      "updatedAt": 1711067493810
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "glob",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import glob\n\n1、返回目录的路径列表\n\npath_list1 = glob.glob('./test_dir/')\n\npath_list2 = glob.glob('./test_dir/*')\n\npath_list3 = glob.glob('./test_dir/*.py')\n\npath_list4 = glob.glob('./test_dir/*/*.py')\n\npath_list5 = glob.glob('./test_dir/**', recursive=True)\n\npath_list6 = glob.glob('./test_dir/**/*.py', recursive=True)"
        }
      ],
      "id": "Ld88RgtY",
      "createdAt": 1715163051153,
      "updatedAt": 1715163101262
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "4ojt5eME",
      "tagsIds": [],
      "description": null,
      "name": "dijkstra",
      "content": [
        {
          "label": "子片段 1",
          "language": "c_cpp",
          "value": "#include <iostream>\n#include <vector>\n#include <algorithm>\nusing namespace std;\n\nint inf = 999999;//不连通的点之间的距离设为无穷大\nlong long int e[10000][10000];\nint dis[10000];//最短距离数组\nint book[10000];//记录下哪些点被选中\n\n//计算单点到全部顶点的距离\nint Dijkstra(int &n, int &m, int &s, vector<vector<int>> &data, int &t)\n{\n\t//初始化任意两点之间的距离数组\n\tfor (int i = 1; i <= n; ++i)\n\t{\n\t\tfor (int j = 1; j <= n; ++j)\n\t\t{\n\t\t\tif (i == j)\n\t\t\t\te[i][j] = 0;\n\t\t\telse\n\t\t\t\te[i][j] = inf;\n\t\t}\n\t}\n\t//把权值加入到任意两点之间的距离数组中\n\tfor (int i = 1; i <= m; ++i)\n\t{\n\t\te[data[i - 1][0]][data[i - 1][1]] = data[i - 1][2];\n\t}\n\tfor (int i = 1; i <= n; ++i)\n\t{\n\t\tif (i != s)\n\t\t{\n\t\t\tdis[i] = e[s][i];//记录源点到其余所有点的最短路径\n\t\t\tbook[i] = 0;//记录哪些点被选取了\n\t\t}\n\t}\n\tint u, min;\n\tfor (int i = 1; i <= n - 1; ++i)\n\t{\n\t\tmin = inf;\n\t\tfor (int j = 1; j <= n; ++j)\n\t\t{\n\t\t\tif (book[j] == 0 && dis[j] < min)//找到源点离还没有被选取的点中的最近顶点\n\t\t\t{\n\t\t\t\tmin = dis[j];\n\t\t\t\tu = j;//记录下最近顶点的位置\n\t\t\t}\n\t\t}\n\t\tbook[u] = 1;\n\t\t/*\n\t\t*例如存在一条从u到v的边，那么可以通过将边u->v添加到尾部来拓展一条从源点到v的路径，\n\t\t*这条路径的长度是dis[u]+e[u][v]。如果这个值比目前已知的dis[v]的值要小，\n\t\t*我们可以用新值来替代当前dis[v]中的值。\n\t\t*/\n\t\tfor (int v = 1; v <= n; ++v)\n\t\t{\n\t\t\tif (e[u][v] < inf)\n\t\t\t{\n\t\t\t\tif (dis[v] > dis[u] + e[u][v])\n\t\t\t\t\tdis[v] = dis[u] + e[u][v];//松弛\n\t\t\t}\n\t\t}\n\t}\n\treturn dis[t];\n}"
        }
      ],
      "id": "BaJf5Dyh",
      "createdAt": 1720877663687,
      "updatedAt": 1720877806903
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "4ojt5eME",
      "tagsIds": [],
      "description": null,
      "name": "floyd",
      "content": [
        {
          "label": "子片段 1",
          "language": "c_cpp",
          "value": "#include <iostream>\n#include <vector>\n#include <algorithm>\nusing namespace std;\n\nint inf = 999999;//不连通的点之间的距离设为无穷大\nlong long int e[10000][10000];\n\n//计算两两顶点之间的最短路径\nvoid Floyd(int &n, int &m, vector<vector<int>> &data)\n{\n\t//初始化任意两点之间的距离数组\n\tfor (int i = 1; i <= n; ++i)\n\t{\n\t\tfor (int j = 1; j <= n; ++j)\n\t\t{\n\t\t\tif (i == j)\n\t\t\t\te[i][j] = 0;\n\t\t\telse\n\t\t\t\te[i][j] = inf;\n\t\t}\n\t}\n\t//把权值加入到任意两点之间的距离数组中\n\tfor (int i = 1; i <= m; ++i)\n\t{\n\t\te[data[i - 1][0]][data[i - 1][1]] = data[i - 1][2];\n\t}\n\t/*\n\t*最开始只允许经过1号顶点进行中转，接下来只允许经过1和2号顶点进行中转……允许经过1~n号所有顶点\n\t*进行中转，求任意两点之间的最短路程。用一句话概括就是：从i号顶点到j号顶点只经过前k号点的最短路程。\n\t*/\n\tfor (int k = 1; k <= n; ++k)\n\t\tfor (int i = 1; i <= n; ++i)\n\t\t\tfor (int j = 1; j <= n; ++j)\n\t\t\t\tif (e[i][j] > e[i][k] + e[k][j])\n\t\t\t\t\te[i][j] = e[i][k] + e[k][j];\n\tfor (int i = 1; i <= n; ++i)\n\t{\n\t\tfor (int j = 1; j <= n; ++j)\n\t\t\tcout << e[i][j] << \" \";\n\t\tcout << endl;\n\t}\n}"
        }
      ],
      "id": "luMgxnlF",
      "createdAt": 1720877674998,
      "updatedAt": 1720877803451
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "4ojt5eME",
      "tagsIds": [],
      "description": null,
      "name": "kruskal & prim",
      "content": [
        {
          "label": "子片段 1",
          "language": "c_cpp",
          "value": "/************************************************************************\nCSDN 勿在浮沙筑高台 http://blog.csdn.net/luoshixian099算法导论--最小生成树（Prim、Kruskal）2016年7月14日\n************************************************************************/\n#include <iostream>\n#include <vector>\n#include <queue>\n#include <algorithm>\nusing namespace std;\n#define INFINITE 0xFFFFFFFF   \n#define VertexData unsigned int  //顶点数据\n#define UINT  unsigned int\n#define vexCounts 6  //顶点数量\nchar vextex[] = { 'A', 'B', 'C', 'D', 'E', 'F' };\nstruct node \n{\n    VertexData data;\n    unsigned int lowestcost;\n}closedge[vexCounts]; //Prim算法中的辅助信息\ntypedef struct \n{\n    VertexData u;\n    VertexData v;\n    unsigned int cost;  //边的代价\n}Arc;  //原始图的边信息\nvoid AdjMatrix(unsigned int adjMat[][vexCounts])  //邻接矩阵表示法\n{\n    for (int i = 0; i < vexCounts; i++)   //初始化邻接矩阵\n        for (int j = 0; j < vexCounts; j++)\n        {\n            adjMat[i][j] = INFINITE;\n        }\n    adjMat[0][1] = 6; adjMat[0][2] = 1; adjMat[0][3] = 5;\n    adjMat[1][0] = 6; adjMat[1][2] = 5; adjMat[1][4] = 3;\n    adjMat[2][0] = 1; adjMat[2][1] = 5; adjMat[2][3] = 5; adjMat[2][4] = 6; adjMat[2][5] = 4;\n    adjMat[3][0] = 5; adjMat[3][2] = 5; adjMat[3][5] = 2;\n    adjMat[4][1] = 3; adjMat[4][2] = 6; adjMat[4][5] = 6;\n    adjMat[5][2] = 4; adjMat[5][3] = 2; adjMat[5][4] = 6;\n}\nint Minmum(struct node * closedge)  //返回最小代价边\n{\n    unsigned int min = INFINITE;\n    int index = -1;\n    for (int i = 0; i < vexCounts;i++)\n    {\n        if (closedge[i].lowestcost < min && closedge[i].lowestcost !=0)\n        {\n            min = closedge[i].lowestcost;\n            index = i;\n        }\n    }\n    return index;\n}\nvoid MiniSpanTree_Prim(unsigned int adjMat[][vexCounts], VertexData s)\n{\n    for (int i = 0; i < vexCounts;i++)\n    {\n        closedge[i].lowestcost = INFINITE;\n    }      \n    closedge[s].data = s;      //从顶点s开始\n    closedge[s].lowestcost = 0;\n    for (int i = 0; i < vexCounts;i++)  //初始化辅助数组\n    {\n        if (i != s)\n        {\n            closedge[i].data = s;\n            closedge[i].lowestcost = adjMat[s][i];\n        }\n    }\n    for (int e = 1; e <= vexCounts -1; e++)  //n-1条边时退出\n    {\n        int k = Minmum(closedge);  //选择最小代价边\n        cout << vextex[closedge[k].data] << \"--\" << vextex[k] << endl;//加入到最小生成树\n        closedge[k].lowestcost = 0; //代价置为0\n        for (int i = 0; i < vexCounts;i++)  //更新v中顶点最小代价边信息\n        {\n            if ( adjMat[k][i] < closedge[i].lowestcost)\n            {\n                closedge[i].data = k;\n                closedge[i].lowestcost = adjMat[k][i];\n            }\n        }\n    }\n}\nvoid ReadArc(unsigned int  adjMat[][vexCounts],vector<Arc> &vertexArc) //保存图的边代价信息\n{\n    Arc * temp = NULL;\n    for (unsigned int i = 0; i < vexCounts;i++)\n    {\n        for (unsigned int j = 0; j < i; j++)\n        {\n            if (adjMat[i][j]!=INFINITE)\n            {\n                temp = new Arc;\n                temp->u = i;\n                temp->v = j;\n                temp->cost = adjMat[i][j];\n                vertexArc.push_back(*temp);\n            }\n        }\n    }\n}\nbool compare(Arc  A, Arc  B)\n{\n    return A.cost < B.cost ? true : false;\n}\nbool FindTree(VertexData u, VertexData v,vector<vector<VertexData> > &Tree)\n{\n    unsigned int index_u = INFINITE;\n    unsigned int index_v = INFINITE;\n    for (unsigned int i = 0; i < Tree.size();i++)  //检查u,v分别属于哪颗树\n    {\n        if (find(Tree[i].begin(), Tree[i].end(), u) != Tree[i].end())\n            index_u = i;\n        if (find(Tree[i].begin(), Tree[i].end(), v) != Tree[i].end())\n            index_v = i;\n    }\n \n    if (index_u != index_v)   //u,v不在一颗树上，合并两颗树\n    {\n        for (unsigned int i = 0; i < Tree[index_v].size();i++)\n        {\n            Tree[index_u].push_back(Tree[index_v][i]);\n        }\n        Tree[index_v].clear();\n        return true;\n    }\n    return false;\n}\nvoid MiniSpanTree_Kruskal(unsigned int adjMat[][vexCounts])\n{\n    vector<Arc> vertexArc;\n    ReadArc(adjMat, vertexArc);//读取边信息\n    sort(vertexArc.begin(), vertexArc.end(), compare);//边按从小到大排序\n    vector<vector<VertexData> > Tree(vexCounts); //6棵独立树\n    for (unsigned int i = 0; i < vexCounts; i++)\n    {\n        Tree[i].push_back(i);  //初始化6棵独立树的信息\n    }\n    for (unsigned int i = 0; i < vertexArc.size(); i++)//依次从小到大取最小代价边\n    {\n        VertexData u = vertexArc[i].u;  \n        VertexData v = vertexArc[i].v;\n        if (FindTree(u, v, Tree))//检查此边的两个顶点是否在一颗树内\n        {\n            cout << vextex[u] << \"---\" << vextex[v] << endl;//把此边加入到最小生成树中\n        }   \n    }\n}\n \nint main()\n{\n    unsigned int  adjMat[vexCounts][vexCounts] = { 0 };\n    AdjMatrix(adjMat);   //邻接矩阵\n    cout << \"Prim :\" << endl;\n    MiniSpanTree_Prim(adjMat,0); //Prim算法，从顶点0开始.\n    cout << \"-------------\" << endl << \"Kruskal:\" << endl;\n    MiniSpanTree_Kruskal(adjMat);//Kruskal算法\n    return 0;\n}"
        }
      ],
      "id": "Xig7I5Yc",
      "createdAt": 1720877781205,
      "updatedAt": 1720877799120
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "fnmatch",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import fnmatch\nimport os\n\nfor file in os.listdir('.'):\n    if fnmatch.fnmatch(file, '*.txt'):\n        print(file)"
        }
      ],
      "id": "Ivhp6eLe",
      "createdAt": 1721203676475,
      "updatedAt": 1721203681413
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "anki_db",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import sqlite3\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n\nif __name__ == \"__main__\":\n    con = sqlite3.connect(\"data.db\")\n    cur = con.cursor()\n    res = cur.execute(\"SELECT tags,sfld from notes\")\n    data = res.fetchall()\n"
        }
      ],
      "id": "60pjlgt8",
      "createdAt": 1722057259380,
      "updatedAt": 1722057293355
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "json objectification",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": ""
        }
      ],
      "id": "kmrtyaUH",
      "createdAt": 1724574340170,
      "updatedAt": 1724574347941
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "logging",
      "content": [
        {
          "label": "basic",
          "language": "python",
          "value": "import logging\n\nLOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\nDATE_FORMAT = \"%m/%d/%Y %H:%M:%S %p\"\n\nlogging.basicConfig(filename='my.log', level=logging.DEBUG, format=LOG_FORMAT, datefmt=DATE_FORMAT)\n\nlogging.debug(\"This is a debug log.\")\nlogging.info(\"This is a info log.\")\nlogging.warning(\"This is a warning log.\")\nlogging.error(\"This is a error log.\")\nlogging.critical(\"This is a critical log.\")"
        },
        {
          "label": "advanced",
          "language": "python",
          "value": "import logging\nimport logging.handlers\nimport datetime\n\nlogger = logging.getLogger('mylogger')\nlogger.setLevel(logging.DEBUG)\n\nrf_handler = logging.handlers.TimedRotatingFileHandler('all.log', when='midnight', interval=1, backupCount=7, atTime=datetime.time(0, 0, 0, 0))\nrf_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n\nf_handler = logging.FileHandler('error.log')\nf_handler.setLevel(logging.ERROR)\nf_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(filename)s[:%(lineno)d] - %(message)s\"))\n\nlogger.addHandler(rf_handler)\nlogger.addHandler(f_handler)\n\nlogger.debug('debug message')\nlogger.info('info message')\nlogger.warning('warning message')\nlogger.error('error message')\nlogger.critical('critical message')"
        },
        {
          "label": "调用subprocess 使用logging打印日志",
          "language": "python",
          "value": "import sys\nimport logging\nfrom logging.handlers import TimedRotatingFileHandler\nimport os\nfrom subprocess import Popen, PIPE, STDOUT\nreload(sys)\nsys.setdefaultencoding('utf8')\n\n\nLOG_FILE_NAME = 'send_snmp_trap.log'\nlogger = logging.getLogger('SenSNMPTrap.py')\nlogger.setLevel(level=logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(funcName)s - %(process)d - %(levelname)s - %(message)s')\nLOG_PATH = os.path.join('/tmp', LOG_FILE_NAME)\n\n# 每天午夜更新日志文件\nhandler = TimedRotatingFileHandler(LOG_PATH, when='midnight', backupCount=3, )\nhandler.setLevel(logging.INFO)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\nconsole.setFormatter(formatter)\n# 输出到屏幕\nlogger.addHandler(console)\n\n\ndef log_subprocess_output(pipe):\n    for line in iter(pipe.readline, b''):  # b'\\n'-separated lines\n        logger.info('got line from subprocess: %r', line)\n\n\ndef run_command(command_line_args):\n    process = Popen(command_line_args, stdout=PIPE, stderr=STDOUT)\n    with process.stdout:\n        log_subprocess_output(process.stdout)\n    exitcode = process.wait()  # 0 means success\n    if exitcode == 0:\n        logger.info('success')\n    else:\n        logger.error(\"failed\")\n\nrun_command(command_line_args=['ls', '/tp/'])"
        }
      ],
      "id": "w2BFB5oy",
      "createdAt": 1724574359923,
      "updatedAt": 1724831165349
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "yolov5_to_coco",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# -*- encoding: utf-8 -*-\n# @File: yolov5_2_coco.py\n# @Author: lijunjie2232\n# @Contact: git@lijunjie2232\n\nimport argparse\nimport json\nimport shutil\nimport time\nimport warnings\nfrom pathlib import Path\n\nimport cv2\nfrom tqdm import tqdm\nimport yaml\n\nclass YOLOV5ToCOCO():\n    def __init__(self, data_dir):\n        self.raw_data_dir = Path(data_dir)\n\n        self.verify_exists(self.raw_data_dir / 'images')\n        self.verify_exists(self.raw_data_dir / 'labels')\n\n        save_dir_name = f'{Path(self.raw_data_dir).name}_COCO_format'\n        self.output_dir = self.raw_data_dir.parent / save_dir_name\n        self.mkdir(self.output_dir)\n\n        self._init_json()\n\n    def __call__(self, mode_list: list):\n        if not mode_list:\n            raise ValueError('mode_list is empty!!')\n\n        for mode in mode_list:\n            # Read the image txt.\n            txt_path = self.raw_data_dir / f'{mode}.txt'\n            self.verify_exists(txt_path)\n            img_list = self.read_txt(txt_path)\n            if mode == 'train':\n                img_list = self.append_bg_img(img_list)\n\n            # Create the directory of saving the new image.\n            save_img_dir = self.output_dir / f'{mode}2017'\n            self.mkdir(save_img_dir)\n\n            # Generate json file.\n            anno_dir = self.output_dir / \"annotations\"\n            self.mkdir(anno_dir)\n\n            save_json_path = anno_dir / f'instances_{mode}2017.json'\n            json_data = self.convert(img_list, save_img_dir, mode)\n\n            self.write_json(save_json_path, json_data)\n        print(f'Successfully convert, detail in {self.output_dir}')\n\n    def _init_json(self):\n        classes_path = self.raw_data_dir / 'classes.txt'\n        data_yaml_path = self.raw_data_dir / 'data.yaml'\n        if self.verify_exists(classes_path, raise_if_ne=False):\n            self.categories = self._get_category(classes_path)\n        elif self.verify_exists(data_yaml_path, raise_if_ne=False):\n            self.categories = self._get_category(data_yaml_path)\n        else:\n            raise(Exception('no dataset info file found'))\n\n        self.type = 'instances'\n        self.annotation_id = 1\n\n        self.cur_year = time.strftime('%Y', time.localtime(time.time()))\n        self.info = {\n            'year': int(self.cur_year),\n            'version': '1.0',\n            'description': 'For object detection',\n            'date_created': self.cur_year,\n        }\n\n        self.licenses = [{\n            'id': 1,\n            'name': 'Apache License v2.0',\n            'url': 'https://github.com/RapidAI/YOLO2COCO/LICENSE',\n        }]\n\n    def append_bg_img(self, img_list):\n        bg_dir = self.raw_data_dir / 'background_images'\n        if bg_dir.exists():\n            bg_img_list = list(bg_dir.iterdir())\n            for bg_img_path in bg_img_list:\n                img_list.append(str(bg_img_path))\n        return img_list\n\n    def _get_category(self, classes_path):\n        if classes_path.name.endswith('txt'):\n            class_list = self.read_txt(classes_path)\n        elif classes_path.name.endswith('yaml'):\n            with open(classes_path, 'r', encoding='utf-8') as f:\n                y = yaml.safe_load(f)\n            class_list = y['names']\n        else:\n            raise(Exception('invalid dataset info file'))\n        categories = []\n        for i, category in enumerate(class_list, 1):\n            categories.append({\n                'supercategory': category,\n                'id': i,\n                'name': category,\n            })\n        return categories\n\n    def convert(self, img_list, save_img_dir, mode):\n        images, annotations = [], []\n        cvt_process = tqdm(img_list, desc=mode)\n        anno_nums = 0\n        for img_id, img_path in enumerate(cvt_process, 1):\n            \n            image_dict = self.get_image_info(img_path, img_id, save_img_dir)\n            images.append(image_dict)\n\n            label_path = self.raw_data_dir / 'labels' / f'{Path(img_path).stem}.txt'\n            annotation = self.get_annotation(label_path,\n                                             img_id,\n                                             image_dict['height'],\n                                             image_dict['width'])\n            if len(annotation) > 0:\n                annotations.extend(annotation)\n                anno_nums += len(annotation)\n            \n            cvt_process.set_postfix(\n                {\n                    'processed': Path(img_path).name,\n                    'anno numbers': anno_nums,\n                }\n            )\n\n        json_data = {\n            'info': self.info,\n            'images': images,\n            'licenses': self.licenses,\n            'type': self.type,\n            'annotations': annotations,\n            'categories': self.categories,\n        }\n        return json_data\n\n    def get_image_info(self, img_path, img_id, save_img_dir):\n        img_path = Path(img_path)\n        if self.raw_data_dir.as_posix() not in img_path.as_posix():\n            # relative path (relative to the raw_data_dir)\n            # e.g. images/images(3).jpg\n            img_path = self.raw_data_dir / img_path\n\n        self.verify_exists(img_path)\n\n        new_img_name = f'{img_id:012d}.jpg'\n        save_img_path = save_img_dir / new_img_name\n        img_src = cv2.imread(str(img_path))\n        if img_path.suffix.lower() == \".jpg\":\n            shutil.copyfile(img_path, save_img_path)\n        else:\n            cv2.imwrite(str(save_img_path), img_src)\n\n        height, width = img_src.shape[:2]\n        image_info = {\n            'date_captured': self.cur_year,\n            'file_name': new_img_name,\n            'id': img_id,\n            'height': height,\n            'width': width,\n        }\n        return image_info\n\n    def get_annotation(self, label_path: Path, img_id, height, width):\n        def get_box_info(vertex_info, height, width):\n            cx, cy, w, h = [float(i) for i in vertex_info]\n\n            cx = cx * width\n            cy = cy * height\n            box_w = w * width\n            box_h = h * height\n\n            # left top\n            x0 = max(cx - box_w / 2, 0)\n            y0 = max(cy - box_h / 2, 0)\n\n            # right bottom\n            x1 = min(x0 + box_w, width)\n            y1 = min(y0 + box_h, height)\n\n            segmentation = [[x0, y0, x1, y0, x1, y1, x0, y1]]\n            bbox = [x0, y0, box_w, box_h]\n            area = box_w * box_h\n            return segmentation, bbox, area\n\n        if not label_path.exists():\n            # annotation = [{\n            #     'segmentation': [],\n            #     'area': 0,\n            #     'iscrowd': 0,\n            #     'image_id': img_id,\n            #     'bbox': [],\n            #     'category_id': -1,\n            #     'id': self.annotation_id,\n            # }]\n            # self.annotation_id += 1\n            # return annotation\n            return []\n\n        annotation = []\n        label_list = self.read_txt(str(label_path))\n        for i, one_line in enumerate(label_list):\n            label_info = one_line.split(' ')\n            if len(label_info) < 5:\n                warnings.warn(\n                    f'The {i+1} line of the {label_path} has been corrupted.')\n                continue\n\n            category_id, vertex_info = label_info[0], label_info[1:]\n            segmentation, bbox, area = get_box_info(vertex_info, height, width)\n            annotation.append({\n                'segmentation': segmentation,\n                'area': area,\n                'iscrowd': 0,\n                'image_id': img_id,\n                'bbox': bbox,\n                'category_id': int(category_id)+1,\n                'id': self.annotation_id,\n            })\n            self.annotation_id += 1\n        return annotation\n\n    @staticmethod\n    def read_txt(txt_path):\n        with open(str(txt_path), 'r', encoding='utf-8') as f:\n            data = list(map(lambda x: x.rstrip('\\n'), f))\n        return data\n\n    @staticmethod\n    def mkdir(dir_path):\n        Path(dir_path).mkdir(parents=True, exist_ok=True)\n\n    @staticmethod\n    def verify_exists(file_path, raise_if_ne=True):\n        file_path = Path(file_path)\n        if not file_path.exists():\n            if raise_if_ne:\n                raise FileNotFoundError(f'The {file_path} is not exists!!!')\n            else:\n                return False\n        else:\n            return True\n\n    @staticmethod\n    def write_json(json_path, content: dict):\n        with open(json_path, 'w', encoding='utf-8') as f:\n            json.dump(content, f, ensure_ascii=False)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser('Datasets converter from YOLOV5 to COCO')\n    parser.add_argument('--data_dir', type=str, default='datasets/YOLOV5',\n                        help='Dataset root path')\n    parser.add_argument('--mode_list', type=str, default='train,val',\n                        help='generate which mode')\n    args = parser.parse_args()\n\n    converter = YOLOV5ToCOCO(args.data_dir)\n    converter(mode_list=args.mode_list.split(','))\n\n"
        }
      ],
      "id": "aM9qIpVf",
      "createdAt": 1724574636984,
      "updatedAt": 1724574654722
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "make_index_4_yolov5",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# -*- encoding: utf-8 -*-\n# @File: makeIndex4yolov5ds.py\n# @Author: lijunjie2232\n# @Contact: git@lijunjie2232\n\nimport random\nimport os\nfrom tqdm import tqdm\nimport argparse\nimport yaml\nfrom pathlib import Path\n\ndef makeIndex(yaml_file=Path('./data.yaml'), out_path=None, ds_type='inner', ds_split='train,val,test'):\n    with open(yaml_file, 'r', encoding='utf-8') as f:\n        yaml_data = yaml.safe_load(f)\n    data_path = Path(yaml_data['path'])\n    train = yaml_data['train']\n    val = yaml_data['val']\n    train_img = []\n    val_img = []\n    if not isinstance(train, list):\n        train = [train]\n    if not isinstance(val, list):\n        val = [val]\n    for d in train:\n        train_img.extend(dataWalker(d, root=data_path))\n    for d in val:\n        val_img.extend(dataWalker(d, root=data_path))\n    saveIndex(train_img, Path(out_path)/\"train.txt\" if out_path else Path(yaml_file).parent/\"train.txt\")\n    saveIndex(val_img, Path(out_path)/\"val.txt\" if out_path else Path(yaml_file).parent/\"val.txt\")\n\n\ndef dataWalker(img_dir, anno_dir=None, root=Path('.')):\n    if not anno_dir:\n        anno_dir = str(img_dir).replace('images', 'labels')\n    img_list = []\n    for file in tqdm(os.listdir(root/img_dir), desc='walking ...', leave=False):\n        if os.path.isfile(os.path.join(root/anno_dir, os.path.splitext(file)[0]+'.txt')):\n            # img_list.append(os.path.abspath(os.path.join(img_dir, file)))\n            img_list.append(Path(os.path.join(img_dir, file)).__str__())\n    return img_list\n\ndef saveIndex(index_list, path):\n    with open(path, 'w') as f:\n        for line in tqdm(index_list, desc='save to '+str(path), leave=False):\n            f.write(line+'\\n')\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('Datasets converter from YOLOV5 to COCO')\n    parser.add_argument('--data_dir', type=str, default='./',\n                        help='Dataset root path')\n    parser.add_argument('--out_dir', type=str, default='',\n                        help='Dataset split result output path')\n    parser.add_argument('--type', type=str, default='inner',\n                        help='Dataset construction type, inner:type dir in images/labels dir; outer:...')\n    parser.add_argument('--data_split', type=str, default='train,val',\n                        help='Dataset split')\n    args = parser.parse_args()\n\n    ROOT = Path('.')\n\n    makeIndex(ROOT/args.data_dir, args.out_dir, ds_type=args.type, ds_split=args.data_split)\n\n"
        }
      ],
      "id": "zXh0Znfi",
      "createdAt": 1724574687043,
      "updatedAt": 1724574709788
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "param & flops",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import torch\nimport torchvision\nfrom thop import profile\n\n# Model\nprint('==> Building model..')\nmodel = torchvision.models.alexnet(pretrained=False)\n\ndummy_input = torch.randn(1, 3, 224, 224)\nflops, params = profile(model, (dummy_input,))\nprint('flops: ', flops, 'params: ', params)\nprint('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))"
        },
        {
          "label": "子片段 2",
          "language": "python",
          "value": "freeze_param = 0\ntrain_param = 0\nfor i in model.parameters():\n    if i.requires_grad:\n        train_param += i.numel()\n    else:\n        freeze_param += i.numel()\nfor name, i in model.named_parameters():\n    if i.requires_grad:\n        train_param += i.numel()\n    else:\n        freeze_param += i.numel()\nprint(\"trainable params: %.2f M\" % (train_param / 1000000.0))\nprint(\"freeze params: %.2f M\" % (freeze_param / 1000000.0))"
        }
      ],
      "id": "AUEnTr0e",
      "createdAt": 1724579916146,
      "updatedAt": 1724776769307
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "time",
      "content": [
        {
          "label": "basic",
          "language": "python",
          "value": "import time\n\nlocaltime = time.localtime(time.time())\nprint \"本地时间为 :\", localtime\n# 本地时间为 : time.struct_time(tm_year=2016, tm_mon=4, tm_mday=7, tm_hour=10, tm_min=3, tm_sec=27, tm_wday=3, tm_yday=98, tm_isdst=0)\n\nlocaltime = time.asctime( time.localtime(time.time()) )\nprint \"本地时间为 :\", localtime\n# 本地时间为 : Thu Apr  7 10:05:21 2016"
        },
        {
          "label": "format",
          "language": "python",
          "value": "import time\n\n# 格式化成2016-03-20 11:45:39形式\nprint time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n# 2016-04-07 10:25:09\n\n# 格式化成Sat Mar 28 22:24:24 2016形式\nprint time.strftime(\"%a %b %d %H:%M:%S %Y\", time.localtime())\n# Thu Apr 07 10:25:09 2016\n\n# 将格式字符串转换为时间戳\na = \"Sat Mar 28 22:24:24 2016\"\nprint time.mktime(time.strptime(a,\"%a %b %d %H:%M:%S %Y\"))\n# 1459175064.0\n\n\"\"\"\npython中时间日期格式化符号：\n\n%y 两位数的年份表示（00-99）\n%Y 四位数的年份表示（000-9999）\n%m 月份（01-12）\n%d 月内中的一天（0-31）\n%H 24小时制小时数（0-23）\n%I 12小时制小时数（01-12）\n%M 分钟数（00-59）\n%S 秒（00-59）\n%a 本地简化星期名称\n%A 本地完整星期名称\n%b 本地简化的月份名称\n%B 本地完整的月份名称\n%c 本地相应的日期表示和时间表示\n%j 年内的一天（001-366）\n%p 本地A.M.或P.M.的等价符\n%U 一年中的星期数（00-53）星期天为星期的开始\n%w 星期（0-6），星期天为星期的开始\n%W 一年中的星期数（00-53）星期一为星期的开始\n%x 本地相应的日期表示\n%X 本地相应的时间表示\n%Z 当前时区的名称\n%% %号本身\n\"\"\""
        },
        {
          "label": "calender",
          "language": "python",
          "value": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n \nimport calendar\n \ncal = calendar.month(2016, 1)\nprint \"以下输出2016年1月份的日历:\"\nprint cal"
        }
      ],
      "id": "WHxAFSQd",
      "createdAt": 1724660133793,
      "updatedAt": 1724660309591
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "yolo_objectify",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import argparse\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport os\nimport shutil\nimport logging as lg\nfrom multiprocessing.pool import ThreadPool\n\nLOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\nlg.basicConfig(level=lg.DEBUG, format=LOG_FORMAT)\n\n\ndef txt_objectify(path: Path):\n    if not path.suffix == \".txt\":\n        return\n    bak_file = path.parent / f\"{path.name}_bak.txt\"\n    if not bak_file.is_file():\n        shutil.copy(path, bak_file)\n    with open(bak_file, \"r\", encoding=\"utf-8\") as old_f:\n        anns = old_f.read().strip(\"\\n\").split(\"\\n\")\n        with open(path, \"w\", encoding=\"utf-8\") as new_f:\n            for ann in anns:\n                ann = ann.strip(\" \").split(\" \")\n                ann[0] = \"0\"\n                new_f.write(\" \".join(ann))\n                new_f.write(\"\\n\")\n\n\ndef dir_walker(dir: Path, filter, func, pool=None):\n    for file in os.listdir(dir):\n        file = dir / file\n        if file.is_dir():\n            dir_walker(file, filter, func)\n        elif filter(file):\n            if pool:\n                pool.apply_async(func=func,args=(file))\n            else:\n                func(file)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"objectification yolo label directory\")\n    parser.add_argument(\"-d\", type=str, required=True)\n    args = parser.parse_args()\n\n    path = Path(args.d)\n\n    filter = lambda file: return file.suffix == \".txt\"\n    \n    pool = ThreadPool(16)\n    \n    dir_walker(path, filter, txt_objectify, pool=pool)\n    \n"
        }
      ],
      "id": "FLBLihaP",
      "createdAt": 1724908462822,
      "updatedAt": 1724908475930
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "v3det(threadpool image check)",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# %%\nimport json\nimport os\nimport shutil\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\nfrom PIL import Image\nfrom multiprocessing.pool import ThreadPool\nfrom itertools import repeat\n\n\n# %%\nDATA = Path(\"./data/\")\nANN_DIR = DATA / \"annotations\"\nOUT_DIR = Path(\"./yolo_format\")\n\n# %%\nwith open(ANN_DIR / \"v3det_2023_v1_train.json\", \"r\", encoding=\"utf-8\") as f:\n    train_json = json.load(f)\n\n\n# %%\ndef img_checker(args):\n    dir, img = args\n    img_file = img[\"file_name\"]\n    if str(img_file).endswith('jpg') or str(img_file).endswith('jpg'):\n        try:\n            Image.open(dir/img_file).load()\n        except:\n            if (dir/img_file).is_file():\n                os.unlik(dir/img_file)\n            img[\"file_name\"] = None\n        finally:\n            return img\n    return None\n\n# %%\nnew_images = []\nnew_id = dict({})\nimg_count = 1\nos.makedirs((OUT_DIR / \"images\" / \"train\"), exist_ok=True)\nwith ThreadPool(32) as pool:\n    results = pool.imap(func=img_checker,iterable= zip(repeat(DATA), train_json[\"images\"]))\n    for i in tqdm(results, total=len(train_json[\"images\"])):\n        if not i:\n            continue\n        if i[\"file_name\"]:\n            img_file = DATA / i[\"file_name\"]\n            new_img_path = OUT_DIR / f\"images/train/{str(img_count).rjust(10, '0')}{img_file.suffix}\"\n            shutil.copy(img_file, new_img_path)\n            this_img = deepcopy(i)\n            this_img[\"file_name\"] = new_img_path.name\n            new_id[this_img[\"id\"]] = img_count\n            this_img[\"id\"] = img_count\n            new_images.append(this_img)\n            img_count += 1\n\n\"\"\"for i in tqdm(train_json[\"images\"]):\n    img_file = DATA / i[\"file_name\"]\n    new_img_path = OUT_DIR / f\"images/train/{str(img_count).rjust(10, '0')}{img_file.suffix}\"\n    try:\n        Image.open(img_file).load()\n    except:\n        print(img_file)\n        os.unlik(img_file)\n        continue\n    if not new_img_path.is_file():\n        shutil.copy(img_file, new_img_path)\n    this_img = deepcopy(i)\n    this_img[\"file_name\"] = new_img_path.name\n    new_id[this_img[\"id\"]] = img_count\n    this_img[\"id\"] = img_count\n    new_images.append(this_img)\n    img_count += 1\"\"\"\n\nnew_annotations = []\nann_count = 0\nfor a in tqdm(train_json[\"annotations\"]):\n    if a[\"image_id\"] in new_id:\n        this_ann = deepcopy(a)\n        this_ann[\"image_id\"] = new_id[this_ann[\"image_id\"]]\n        this_ann[\"id\"] = ann_count\n        new_annotations.append(this_ann)\n        ann_count += 1\n\nnew_train_json = deepcopy(train_json)\nnew_train_json[\"images\"] = new_images\nnew_train_json[\"annotations\"] = new_annotations\n\nwith open(OUT_DIR / \"annotations\" / \"instances_train.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(new_train_json, f)\n\n# %%\ni\n\n# %%\na\n\n\n# %%\nthis_img\n\n\n# %%\nthis_ann\n\n\n# %%\nwith open(ANN_DIR / \"v3det_2023_v1_val.json\", \"r\", encoding=\"utf-8\") as f:\n    val_json = json.load(f)\n\n\n# %%\nnew_images = []\nnew_id = dict({})\nimg_count = 1\nos.makedirs((OUT_DIR / \"images\" / \"val\"), exist_ok=True)\nwith ThreadPool(32) as pool:\n    results = pool.imap(target=img_checker,iterable= zip(repeat(DATA), val_json[\"images\"]))\n    for i in tqdm(results, total=len(val_json[\"images\"])):\n        if not i:\n            continue\n        if i[\"file_name\"]:\n            img_file = DATA / i[\"file_name\"]\n            new_img_path = OUT_DIR / f\"images/val/{str(img_count).rjust(10, '0')}{img_file.suffix}\"\n            shutil.copy(img_file, new_img_path)\n            this_img = deepcopy(i)\n            this_img[\"file_name\"] = new_img_path.name\n            new_id[this_img[\"id\"]] = img_count\n            this_img[\"id\"] = img_count\n            new_images.append(this_img)\n            img_count += 1\n\n\"\"\"for i in tqdm(val_json[\"images\"]):\n    img_file = DATA / i[\"file_name\"]\n    new_img_path = OUT_DIR / f\"images/val/{str(img_count).rjust(10, '0')}{img_file.suffix}\"\n    try:\n        image.open(img_file).load()\n    except:\n        print(img_file)\n        os.unlik(img_file)\n        continue\n    if not new_img_path.is_file():\n        shutil.copy(img_file, new_img_path)\n    this_img = deepcopy(i)\n    this_img[\"file_name\"] = new_img_path.name\n    new_id[this_img[\"id\"]] = img_count\n    this_img[\"id\"] = img_count\n    new_images.append(this_img)\n    img_count += 1\"\"\"\n\nnew_annotations = []\nann_count = 0\nfor a in tqdm(val_json[\"annotations\"]):\n    if a[\"image_id\"] in new_id:\n        this_ann = deepcopy(a)\n        this_ann[\"image_id\"] = new_id[this_ann[\"image_id\"]]\n        this_ann[\"id\"] = ann_count\n        new_annotations.append(this_ann)\n        ann_count += 1\n\nnew_val_json = deepcopy(val_json)\nnew_val_json[\"images\"] = new_images\nnew_val_json[\"annotations\"] = new_annotations\n\nwith open(OUT_DIR / \"annotations\" / \"instances_val.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(new_val_json, f)\n\n\n\n\n"
        }
      ],
      "id": "iXUs53Ff",
      "createdAt": 1724919224198,
      "updatedAt": 1724919274735
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "image checker",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "# %%\nimport json\nimport os\nimport shutil\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom PIL import Image\nfrom multiprocessing.pool import ThreadPool\nfrom itertools import repeat\n\n\n# %%\nDATA = Path(\"./data/\")\nANN_DIR = DATA / \"annotations\"\nOUT_DIR = Path(\"./yolo_format\")\n\n# %%\nwith open(ANN_DIR / \"v3det_2023_v1_train.json\", \"r\", encoding=\"utf-8\") as f:\n    train_json = json.load(f)\n\n\n# %%\ndef img_checker(args):\n    dir, img = args\n    img_file = img[\"file_name\"]\n    if str(img_file).endswith('jpg') or str(img_file).endswith('jpg'):\n        try:\n            Image.open(dir / img_file).verify()\n            img = Image.open(dir / img_file)\n            img.load()\n        except:\n            if (dir/img_file).is_file():\n                os.unlik(dir/img_file)\n            img[\"file_name\"] = None\n        finally:\n            return img\n    return None\n\n# %%\nnew_images = []\nnew_id = dict({})\nimg_count = 1\nos.makedirs((OUT_DIR / \"images\" / \"train\"), exist_ok=True)\nwith ThreadPool(32) as pool:\n    results = pool.imap(func=img_checker,iterable= zip(repeat(DATA), train_json[\"images\"]))\n    for i in tqdm(results, total=len(train_json[\"images\"])):\n        if not i:\n            continue\n        if i[\"file_name\"]:\n            img_file = DATA / i[\"file_name\"]\n            new_img_path = OUT_DIR / f\"images/train/{str(img_count).rjust(10, '0')}{img_file.suffix}\"\n            shutil.copy(img_file, new_img_path)\n            this_img = deepcopy(i)\n            this_img[\"file_name\"] = new_img_path.name\n            new_id[this_img[\"id\"]] = img_count\n            this_img[\"id\"] = img_count\n            new_images.append(this_img)\n            img_count += 1\n"
        }
      ],
      "id": "OaCbxc8o",
      "createdAt": 1725756511234,
      "updatedAt": 1725770181958
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "pytorch ddp",
      "content": [
        {
          "label": "train",
          "language": "python",
          "value": "导入\ntorch.multiprocessing 是围绕 Python 原生多处理的 PyTorch 包装器\n\n分布式进程组包含所有可以相互通信和同步的进程。\n\nimport torch\nimport torch.nn.functional as F\nfrom utils import MyTrainDataset\n\n+ import torch.multiprocessing as mp\n+ from torch.utils.data.distributed import DistributedSampler\n+ from torch.nn.parallel import DistributedDataParallel as DDP\n+ from torch.distributed import init_process_group, destroy_process_group\n+ import os\n构建进程组\n首先，在初始化组进程之前，调用 set_device，它为每个进程设置默认 GPU。这对于防止 GPU:0 上的挂起或过度内存使用至关重要。\n\n进程组可以通过 TCP（默认）或从共享文件系统初始化。阅读有关 进程组初始化 的更多信息\n\ninit_process_group 初始化分布式进程组。\n\n了解有关 选择 DDP 后端 的更多信息\n\n+ def ddp_setup(rank: int, world_size: int):\n+   \"\"\"\n+   Args:\n+       rank: Unique identifier of each process\n+      world_size: Total number of processes\n+   \"\"\"\n+   os.environ[\"MASTER_ADDR\"] = \"localhost\"\n+   os.environ[\"MASTER_PORT\"] = \"12355\"\n+   torch.cuda.set_device(rank)\n+   init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n构建 DDP 模型\n- self.model = model.to(gpu_id)\n+ self.model = DDP(model, device_ids=[gpu_id])\n分发输入数据\nDistributedSampler 将输入数据跨所有分布式进程进行分块。\n\n每个进程将接收一个包含 32 个样本的输入批次；有效批次大小为 32 * nprocs，使用 4 个 GPU 时为 128。\n\ntrain_data = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n-   shuffle=True,\n+   shuffle=False,\n+   sampler=DistributedSampler(train_dataset),\n)\n在每个 epoch 开始时调用 DistributedSampler 上的 set_epoch() 方法对于使跨多个 epoch 的混洗正常工作是必要的。否则，将在每个 epoch 中使用相同的排序。\n\ndef _run_epoch(self, epoch):\n    b_sz = len(next(iter(self.train_data))[0])\n+   self.train_data.sampler.set_epoch(epoch)\n    for source, targets in self.train_data:\n      ...\n      self._run_batch(source, targets)\n保存模型检查点\n我们只需要从一个进程保存模型检查点。如果没有这个条件，每个进程都会保存其相同模式的副本。阅读有关使用 DDP 保存和加载模型的更多信息 此处\n\n- ckp = self.model.state_dict()\n+ ckp = self.model.module.state_dict()\n...\n...\n- if epoch % self.save_every == 0:\n+ if self.gpu_id == 0 and epoch % self.save_every == 0:\n  self._save_checkpoint(epoch)\n警告\n\n集体调用 是在所有分布式进程上运行的函数，它们用于将某些状态或值收集到特定进程。集体调用要求所有等级都运行集体代码。在这个例子中，_save_checkpoint 不应该有任何集体调用，因为它只在 rank:0 进程上运行。如果您需要进行任何集体调用，则应在 if self.gpu_id == 0 检查之前进行。\n\n运行分布式训练作业\n包括新的参数 rank（替换 device）和 world_size。\n\nrank 是在调用 mp.spawn 时由 DDP 自动分配的。\n\nworld_size 是整个训练作业中的进程数量。对于 GPU 训练，这对应于使用的 GPU 数量，并且每个进程都在专用 GPU 上工作。\n\n- def main(device, total_epochs, save_every):\n+ def main(rank, world_size, total_epochs, save_every):\n+  ddp_setup(rank, world_size)\n   dataset, model, optimizer = load_train_objs()\n   train_data = prepare_dataloader(dataset, batch_size=32)\n-  trainer = Trainer(model, train_data, optimizer, device, save_every)\n+  trainer = Trainer(model, train_data, optimizer, rank, save_every)\n   trainer.train(total_epochs)\n+  destroy_process_group()\n\nif __name__ == \"__main__\":\n   import sys\n   total_epochs = int(sys.argv[1])\n   save_every = int(sys.argv[2])\n-  device = 0      # shorthand for cuda:0\n-  main(device, total_epochs, save_every)\n+  world_size = torch.cuda.device_count()\n+  mp.spawn(main, args=(world_size, total_epochs, save_every,), nprocs=world_size)"
        },
        {
          "label": "inference",
          "language": "python",
          "value": "import torch\nimport torch.distributed as dist\nimport json\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n# 模拟推理结果\ndef inference_on_gpu(model, data_loader, device):\n    model.eval()\n    all_predictions = []\n    with torch.no_grad():\n        for images, image_ids in data_loader:\n            images = images.to(device)\n            outputs = model(images)  # 假设模型输出包含预测框和类别\n\n            # 将输出转换为 COCO 格式的预测结果\n            for i, output in enumerate(outputs):\n                pred_boxes = output['boxes'].cpu().numpy()\n                pred_scores = output['scores'].cpu().numpy()\n                pred_labels = output['labels'].cpu().numpy()\n\n                # 每张图片的预测框\n                for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n                    all_predictions.append({\n                        \"image_id\": image_ids[i].item(),\n                        \"category_id\": label.item(),\n                        \"bbox\": box.tolist(),  # COCO 格式的 [x_min, y_min, width, height]\n                        \"score\": score.item()\n                    })\n    return all_predictions\n\n# 收集所有 GPU 上的推理结果\ndef gather_predictions(predictions):\n    # 如果是分布式训练，需要收集每个 GPU 的结果\n    all_predictions = [None for _ in range(dist.get_world_size())]\n    dist.all_gather_object(all_predictions, predictions)  # 收集每个 GPU 的预测结果\n    # 展平收集到的预测结果\n    all_predictions = [item for sublist in all_predictions for item in sublist]\n    return all_predictions\n\n# 计算 COCO mAP\ndef evaluate_predictions(coco_gt, all_predictions, output_file='predictions.json'):\n    with open(output_file, 'w') as f:\n        json.dump(all_predictions, f)\n\n    coco_dt = coco_gt.loadRes(output_file)\n    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n\n# 主推理与评估流程\ndef main():\n    # 初始化分布式环境\n    dist.init_process_group(backend='nccl')\n\n    # 设置设备\n    device = torch.device(f'cuda:{dist.get_rank()}')\n\n    # 加载模型和数据\n    model = YourModel().to(device)\n    data_loader = YourDataLoader()  # 假设 data_loader 返回 (images, image_ids)\n    \n    # COCO ground truth 数据\n    coco_gt = COCO(annotation_file='instances_val2017.json')\n\n    # 每个 GPU 执行推理\n    predictions = inference_on_gpu(model, data_loader, device)\n\n    # 收集所有 GPU 的推理结果\n    all_predictions = gather_predictions(predictions)\n\n    # 只有 rank 0 的进程负责计算 mAP\n    if dist.get_rank() == 0:\n        evaluate_predictions(coco_gt, all_predictions)\n\n    # 结束分布式进程组\n    dist.barrier()\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ],
      "id": "PfEmpLuD",
      "createdAt": 1725932493707,
      "updatedAt": 1726814950100
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "memory_profiler",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import numpy as np\nfrom memory_profiler import profile\n \n@profile\ndef demo():\n    a = np.random.rand(10000000)\n    b = np.random.rand(10000000)\n    \n    a_ = a[a < b]\n    b_ = b[a < b]\n    \n    del a, b\n \n    return a_, b_\n \n \nif __name__ == '__main__':\n    demo()"
        }
      ],
      "id": "1mZKVtdM",
      "createdAt": 1726649105037,
      "updatedAt": 1726649118980
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "@timecalc",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "def timecalc(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(f\"[function]: {func.__name__}, [time]: {end - start}ms\")\n        return result\n    return wrapper"
        }
      ],
      "id": "JWFfVT6n",
      "createdAt": 1726819216801,
      "updatedAt": 1726819221854
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "Queue",
      "content": [
        {
          "label": "get&put",
          "language": "plain_text",
          "value": "\"\"\"\nPython 中的消息队列\nPython 提供了 queue 模块来实现线程之间的消息队列。queue.Queue 是线程安全的，允许多个线程安全地访问和修改队列中的数据。\n\nqueue.Queue 的基本用法\nqueue.Queue 通过两个主要方法实现线程安全的队列操作：\n\nput(item)：将数据项放入队列。\nget()：从队列中取出数据项。如果队列为空，它会阻塞直到有数据可取。\n\"\"\"\n\nimport threading\nimport queue\nimport time\n\n# 创建队列对象\nq = queue.Queue()\n\n# 生产者线程\ndef producer():\n    for i in range(5):\n        item = f\"item_{i}\"\n        print(f\"Producing {item}\")\n        q.put(item)\n        time.sleep(1)\n\n# 消费者线程\ndef consumer():\n    while True:\n        item = q.get()\n        if item is None:\n            break\n        print(f\"Consuming {item}\")\n        q.task_done()\n\n# 创建线程\nproducer_thread = threading.Thread(target=producer)\nconsumer_thread = threading.Thread(target=consumer)\n\n# 启动线程\nproducer_thread.start()\nconsumer_thread.start()\n\n# 等待生产者结束\nproducer_thread.join()\n\n# 向队列发送终止信号\nq.put(None)\n\n# 等待消费者结束\nconsumer_thread.join()\n\nprint(\"All tasks finished\")\n\"\"\"\n分析：\n生产者线程：负责将数据项放入队列。这里每隔 1 秒向队列中添加一个数据项。\n消费者线程：负责从队列中取出数据并处理。如果队列为空，q.get() 会阻塞，直到有新数据项进入队列。当所有生产任务完成后，通过向队列中添加 None 来通知消费者线程结束。\n线程通信：queue.Queue 在多个线程之间安全地传递数据，避免了手动加锁的复杂性。\n\"\"\""
        },
        {
          "label": "Full&Empty",
          "language": "python",
          "value": "\"\"\"\n阻塞与非阻塞队列\n队列的 put() 和 get() 方法默认是阻塞的，但你可以通过设置 block=False 来让它们变为非阻塞模式。如果在非阻塞模式下调用 get() 时队列为空，或者 put() 时队列满了，它们会抛出 queue.Empty 或 queue.Full 异常。\n\n非阻塞队列示例：\n\"\"\"\n\nimport queue\n\nq = queue.Queue(maxsize=3)\n\n# 非阻塞地放入队列\ntry:\n    q.put(\"item\", block=False)\nexcept queue.Full:\n    print(\"Queue is full!\")\n\n# 非阻塞地获取队列\ntry:\n    item = q.get(block=False)\nexcept queue.Empty:\n    print(\"Queue is empty!\")"
        },
        {
          "label": "join&task_done",
          "language": "python",
          "value": "\"\"\"\nqueue.Queue 的其他方法\nq.task_done()：当消费者线程完成对某个任务的处理时，调用此方法通知队列该任务已完成。\nq.join()：阻塞主线程，直到队列中的所有任务都已完成。它会等待所有任务都调用 task_done()。\n\"\"\"\nimport threading\nimport queue\n\nq = queue.Queue()\n\ndef producer():\n    for i in range(5):\n        q.put(i)\n        print(f\"Produced {i}\")\n\ndef consumer():\n    while True:\n        item = q.get()\n        print(f\"Consumed {item}\")\n        q.task_done()\n\n# 启动生产者和消费者线程\nthreading.Thread(target=producer).start()\nthreading.Thread(target=consumer, daemon=True).start()\n\n# 等待所有任务完成\nq.join()\nprint(\"All tasks are done\")"
        },
        {
          "label": "子片段 4",
          "language": "python",
          "value": "\"\"\"\nPython queue 模块还提供了其他几种类型的队列：\n\nqueue.LifoQueue：后进先出（LIFO）的队列，类似于栈。\nqueue.PriorityQueue：优先级队列，数据项会按照优先级顺序排列。\n优先级队列示例：\n\"\"\"\nimport queue\n\npq = queue.PriorityQueue()\n\n# 将数据按照优先级放入队列\npq.put((2, \"Medium priority\"))\npq.put((1, \"High priority\"))\npq.put((3, \"Low priority\"))\n\n# 按照优先级顺序获取数据\nwhile not pq.empty():\n    priority, task = pq.get()\n    print(f\"Processing task with priority {priority}: {task}\")"
        },
        {
          "label": "子片段 5",
          "language": "python",
          "value": ""
        },
        {
          "label": "子片段 6",
          "language": "python",
          "value": ""
        }
      ],
      "id": "YcCA6Xx6",
      "createdAt": 1726991761016,
      "updatedAt": 1726993316352
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "re replace",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "re.sub(\"<.*?>\", \"\", word)"
        }
      ],
      "id": "zW1UeYgh",
      "createdAt": 1727353216088,
      "updatedAt": 1727425970531
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "cuda time",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "torch.cuda.synchronize()\nstart = time()\nmodel(x)\ntorch.cuda.synchronize()\nend = time()\nprint(end - start, \"ms\")"
        }
      ],
      "id": "HVTNDdUU",
      "createdAt": 1727425732121,
      "updatedAt": 1727425739504
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "cude mem",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "import torch\nfrom torchvision import models\nfrom memory_profiler import profile\nfrom inspect import unwrap\nfrom time import time\n\n\n@profile\ndef model_test():\n    device = torch.device(\"cuda:0\")\n    model = models.resnet50().cuda()\n    print(torch.cuda.memory_allocated() / 1024**2, \"MB\")\n    model = model.cpu()\n    print(torch.cuda.max_memory_allocated() / 1024**2, \"MB\")\n    print(torch.cuda.memory_summary())\n    model = model.cuda()\n\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        # execution_trace_observer=(\n        #     torch.profiler.ExecutionTraceObserver().register_callback(\"./execution_trace.json\")\n        # ),\n        record_shapes=True,\n        profile_memory=True,  # 启用显存分析\n        with_stack=True,\n    ) as prof:\n        x = torch.randn(1, 3, 224, 224).cuda()\n        model(x)\n    print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n\n    x = torch.randn(32, 3, 224, 224).cuda()\n    torch.cuda.synchronize()\n    start = time()\n    model(x)\n    torch.cuda.synchronize()\n    end = time()\n    print(end - start, \"ms\")\n\n\nif __name__ == \"__main__\":\n    model_test()\n    unwrap(model_test)()\n"
        }
      ],
      "id": "7aNFOFbC",
      "createdAt": 1727425948490,
      "updatedAt": 1727425960495
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "pytorch cudnn enable",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import torch\ntorch.backends.cudnn.enabled = True\ntorch.backend.cudnn.benchmark=True"
        }
      ],
      "id": "9doO_wAe",
      "createdAt": 1727507206242,
      "updatedAt": 1727507218048
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "whisper inference with ddp",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import whisper\nfrom whisper.transcribe import *\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed import init_process_group, barrier\nimport os\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport argparse\n\nos.environ[\"MKL_NUM_THREADS\"] = \"1024\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1024\"\ntorch.set_num_threads(1024)\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\n\nLOCAL_RANK = int(\n    os.getenv(\"LOCAL_RANK\", 0)\n)  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv(\"RANK\", 0))\nWORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\n\n\nclass input_files(Dataset):\n    def __init__(self, path, valid_suffix=[\".acc\", \".mp3\", \".wav\"]):\n        self.path = path\n        self.files = (self.path / file for file in os.listdir(path))\n        self.files = [\n            f.__str__() for f in filter(lambda f: f.suffix in valid_suffix, self.files)\n        ]\n\n    def __getitem__(self, index):\n        return self.files[index]\n\n    def __len__(self):\n        return len(self.files)\n\n\ndef ddp_setup(rank: int, world_size: int):\n    \"\"\"\n    Args:\n        rank: Unique identifier of each process\n    world_size: Total number of processes\n    \"\"\"\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\n\ndef get_args():\n    # fmt: off\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # parser.add_argument(\"audio\", nargs=\"+\", type=str, help=\"audio file(s) to transcribe\")\n    parser.add_argument(\"--model\", default=\"small\", type=str, help=\"name of the Whisper model to use\")\n    parser.add_argument(\"--model_dir\", type=str, default=None, help=\"the path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=\"./output\", help=\"directory to save the outputs\")\n    parser.add_argument(\"--output_format\", \"-f\", type=str, default=\"all\", choices=[\"txt\", \"vtt\", \"srt\", \"tsv\", \"json\", \"all\"], help=\"format of the output file; if not specified, all available formats will be produced\")\n    parser.add_argument(\"--verbose\", type=str2bool, default=True, help=\"whether to print out the progress and debug messages\")\n\n    parser.add_argument(\"--task\", type=str, default=\"transcribe\", choices=[\"transcribe\", \"translate\"], help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=None, choices=sorted(LANGUAGES.keys()) + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]), help=\"language spoken in the audio, specify None to perform language detection\")\n\n    parser.add_argument(\"--temperature\", type=float, default=0, help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=5, help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=5, help=\"number of beams in beam search, only applicable when temperature is zero\")\n    parser.add_argument(\"--patience\", type=float, default=None, help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=None, help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default\")\n\n    parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=True, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n    parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n    parser.add_argument(\"--word_timestamps\", type=str2bool, default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'“¿([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.。,，!！?？:：”)]}、\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n    parser.add_argument(\"--highlight_words\", type=str2bool, default=False, help=\"(requires --word_timestamps True) underline each word as it is spoken in srt and vtt\")\n    parser.add_argument(\"--max_line_width\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of characters in a line before breaking the line\")\n    parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")\n    parser.add_argument(\"--max_words_per_line\", type=optional_int, default=None, help=\"(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment\")\n    parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n    parser.add_argument(\"--clip_timestamps\", type=str, default=\"0\", help=\"comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process, where the last end timestamp defaults to the end of the file\")\n    parser.add_argument(\"--hallucination_silence_threshold\", type=optional_float, help=\"(requires --word_timestamps True) skip silent periods longer than this threshold (in seconds) when a possible hallucination is detected\")\n\n    return parser.parse_args().__dict__\n\n\nif __name__ == \"__main__\":\n    ddp_setup(RANK, WORLD_SIZE)\n    input_dir = Path(\"input\")\n    output_dir = Path(\"output\")\n    output_dir.mkdir(exist_ok=True)\n\n    args = get_args()\n    model_name = \"large-v3\"\n    model_dir = args[\"model_dir\"]\n    output_format = \"all\"\n    device = \"cuda\"\n    args[\"language\"] = \"Japanese\"\n\n    args.pop(\"model\")\n    args.pop(\"model_dir\")\n    args.pop(\"output_dir\")\n    args.pop(\"output_format\")\n    args.pop(\"device\")\n    args.pop(\"threads\")\n\n    model = whisper.load_model(\"large-v3\", device=\"cuda\", download_root=model_dir)\n    temperature = args.pop(\"temperature\")\n    if (increment := args.pop(\"temperature_increment_on_fallback\")) is not None:\n        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, increment))\n    else:\n        temperature = [temperature]\n\n    writer = get_writer(output_format, output_dir)\n    word_options = [\n        \"highlight_words\",\n        \"max_line_count\",\n        \"max_line_width\",\n        \"max_words_per_line\",\n    ]\n    args[\"word_timestamps\"] = True\n    if not args[\"word_timestamps\"]:\n        for option in word_options:\n            assert args[option], Exception(\n                f\"--{option} requires --word_timestamps True\"\n            )\n    if args[\"max_line_count\"] and not args[\"max_line_width\"]:\n        warnings.warn(\"--max_line_count has no effect without --max_line_width\")\n    if args[\"max_words_per_line\"] and args[\"max_line_width\"]:\n        warnings.warn(\"--max_words_per_line has no effect with --max_line_width\")\n    writer_args = {arg: args.pop(arg) for arg in word_options}\n\n    dataset = input_files(input_dir)\n    loop = DataLoader(\n        dataset,\n        batch_size=1,\n        sampler=DistributedSampler(dataset, shuffle=False),\n    )\n    if not RANK:\n        loop = tqdm(loop)\n    for audio_path in loop:\n        assert len(audio_path) == 1\n        audio_path = audio_path[0]\n        result = transcribe(\n            model, audio_path.__str__(), temperature=temperature, **args\n        )\n        writer(result, audio_path, **writer_args)\n        # try:\n        #     result = transcribe(model, audio_path.__str__(), temperature=temperature, **args)\n        #     writer(result, audio_path, **writer_args)\n        # except Exception as e:\n        #     traceback.print_exc()\n        #     print(f\"Skipping {audio_path} due to {type(e).__name__}: {str(e)}\")\n    barrier()\n"
        }
      ],
      "id": "v8waXzoJ",
      "createdAt": 1727510394313,
      "updatedAt": 1727511239340
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "weWtbZI4",
      "tagsIds": [],
      "description": null,
      "name": "mp3 extractor",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import subprocess\nfrom pathlib import Path\nimport os\nimport sys\nfrom multiprocessing.pool import Pool\n\n\ndef mp3_extractor(input, output):\n    subprocess.run(\n        [\n            \"ffmpeg.exe\",\n            \"-i\",\n            input,\n            \"-vn\",\n            \"-c:a\",\n            \"mp3\",\n            output,\n        ]\n    )\n\n\nif __name__ == \"__main__\":\n    ROOT = Path(__file__).parent.parent\n    mp3_dir = ROOT / \"mp3\"\n    mp3_dir.mkdir(exist_ok=True)\n    with Pool(16) as pool:\n        for file in os.listdir(ROOT):\n            file = ROOT / file\n            if file.suffix != \".mp4\":\n                continue\n            input = file.absolute().__str__()\n            output = (mp3_dir / f\"{file.stem}.mp3\").absolute().__str__()\n            pool.apply_async(mp3_extractor, args=(input, output))\n        pool.close()\n        pool.join()\n"
        }
      ],
      "id": "wthzwaeG",
      "createdAt": 1727512632091,
      "updatedAt": 1727512640972
    },
    {
      "isDeleted": true,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "未命名程式碼片段",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": ""
        }
      ],
      "id": "YOzeI46J",
      "createdAt": 1729164388322,
      "updatedAt": 1731240823018
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "hamming",
      "content": [
        {
          "label": "子片段 1",
          "language": "python",
          "value": "import torch\n\ndef hamming_distance_batchwise(hamming_codes):\n    \"\"\"\n    计算 m 个二进制向量两两之间的海明距离\n    :param hamming_codes: 二进制向量张量，形状为 [m, embedding_size]\n    :return: 海明距离矩阵，形状为 [m, m]\n    \"\"\"\n    m = hamming_codes.size(0)\n    \n    # 扩展维度，使每个向量可以与其他所有向量进行比较\n    # 形状从 [m, embedding_size] 变成 [m, 1, embedding_size] 和 [1, m, embedding_size]\n    hamming_codes_1 = hamming_codes.unsqueeze(1)  # [m, 1, embedding_size]\n    hamming_codes_2 = hamming_codes.unsqueeze(0)  # [1, m, embedding_size]\n    \n    # 进行逐位异或操作，得到异或结果矩阵，形状为 [m, m, embedding_size]\n    xor_result = torch.bitwise_xor(hamming_codes_1, hamming_codes_2)\n    \n    # 统计异或结果中 1 的数量，即海明距离\n    # 最后结果是 [m, m] 的矩阵，其中每个元素是对应向量之间的海明距离\n    hamming_distances = torch.sum(xor_result, dim=-1)\n    \n    return hamming_distances\n\n# 测试\nhamming_codes = torch.tensor([[0, 0, 1, 1, 0, 1, 1], \n                              [1, 0, 0, 1, 1, 0, 1], \n                              [1, 1, 1, 0, 0, 0, 1]], dtype=torch.int64)\n\n# 计算两两之间的海明距离矩阵\nhamming_distances = hamming_distance_batchwise(hamming_codes)\nprint(\"Hamming distance matrix:\\n\", hamming_distances)\n"
        }
      ],
      "id": "1V9l10Wh",
      "createdAt": 1729164391321,
      "updatedAt": 1729164399413
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "PoO2O3Rm",
      "tagsIds": [],
      "description": null,
      "name": "Date/SimpleDateFormat/Calendar",
      "content": [
        {
          "label": "format",
          "language": "markdown",
          "value": "#### Date 对象创建以后，可以调用下面的方法。\n\n| 序号 | 方法和描述                                                   |\n| :--- | :----------------------------------------------------------- |\n| 1    | **boolean after(Date date)** 若当调用此方法的Date对象在指定日期之后返回true,否则返回false。 |\n| 2    | **boolean before(Date date)** 若当调用此方法的Date对象在指定日期之前返回true,否则返回false。 |\n| 3    | **Object clone( )** 返回此对象的副本。                       |\n| 4    | **int compareTo(Date date)** 比较当调用此方法的Date对象和指定日期。两者相等时候返回0。调用对象在指定日期之前则返回负数。调用对象在指定日期之后则返回正数。 |\n| 5    | **int compareTo(Object obj)** 若obj是Date类型则操作等同于compareTo(Date) 。否则它抛出ClassCastException。 |\n| 6    | **boolean equals(Object date)** 当调用此方法的Date对象和指定日期相等时候返回true,否则返回false。 |\n| 7    | **long getTime( )** 返回自 1970 年 1 月 1 日 00:00:00 GMT 以来此 Date 对象表示的毫秒数。 |\n| 8    | **int hashCode( )**  返回此对象的哈希码值。                  |\n| 9    | **void setTime(long time)**   用自1970年1月1日00:00:00 GMT以后time毫秒数设置时间和日期。 |\n| 10   | **String toString( )** 把此 Date 对象转换为以下形式的 String： dow mon dd hh:mm:ss zzz yyyy 其中： dow 是一周中的某一天 (Sun, Mon, Tue, Wed, Thu, Fri, Sat)。 |\n\n#### printf格式\n| 转 换 符 | 说  明                      | 示  例                           |\n| :------- | :-------------------------- | :------------------------------- |\n| c        | 包括全部日期和时间信息      | 星期六 十月 27 14:21:20 CST 2007 |\n| F        | \"年-月-日\"格式              | 2007-10-27                       |\n| D        | \"月/日/年\"格式              | 10/27/07                         |\n| r        | \"HH:MM:SS PM\"格式（12时制） | 02:25:51 下午                    |\n| T        | \"HH:MM:SS\"格式（24时制）    | 14:28:16                         |\n| R        | \"HH:MM\"格式（24时制）       | 14:28                            |\n\n#### SimpleDateFormat 格式\n| **字母** | **描述**                 | **示例**                |\n| :------- | :----------------------- | :---------------------- |\n| G        | 纪元标记                 | AD                      |\n| y        | 四位年份                 | 2001                    |\n| M        | 月份                     | July or 07              |\n| d        | 一个月的日期             | 10                      |\n| h        | A.M./P.M. (1~12)格式小时 | 12                      |\n| H        | 一天中的小时 (0~23)      | 22                      |\n| m        | 分钟数                   | 30                      |\n| s        | 秒数                     | 55                      |\n| S        | 毫秒数                   | 234                     |\n| E        | 星期几                   | Tuesday                 |\n| D        | 一年中的日子             | 360                     |\n| F        | 一个月中第几周的周几     | 2 (second Wed. in July) |\n| w        | 一年中第几周             | 40                      |\n| W        | 一个月中第几周           | 1                       |\n| a        | A.M./P.M. 标记           | PM                      |\n| k        | 一天中的小时(1~24)       | 24                      |\n| K        | A.M./P.M. (0~11)格式小时 | 10                      |\n| z        | 时区                     | Eastern Standard Time   |\n| '        | 文字定界符               | Delimiter               |\n| \"        | 单引号                   | `                       |\n\n#### Calendar类对象字段类型\n\nCalendar类中用以下这些常量表示不同的意义，jdk内的很多类其实都是采用的这种思想\n\n| 常量                  | 描述                           |\n| :-------------------- | :----------------------------- |\n| Calendar.YEAR         | 年份                           |\n| Calendar.MONTH        | 月份                           |\n| Calendar.DATE         | 日期                           |\n| Calendar.DAY_OF_MONTH | 日期，和上面的字段意义完全相同 |\n| Calendar.HOUR         | 12小时制的小时                 |\n| Calendar.HOUR_OF_DAY  | 24小时制的小时                 |\n| Calendar.MINUTE       | 分钟                           |\n| Calendar.SECOND       | 秒                             |\n| Calendar.DAY_OF_WEEK  | 星期几                         |"
        },
        {
          "label": "Date",
          "language": "java",
          "value": "import java.util.Date;\n\npublic class DateDemo {\n \n    public static void main(String[] args) {\n        // 初始化 Date 对象\n        Date date = new Date();\n\n        //c的使用  \n        System.out.printf(\"全部日期和时间信息：%tc%n\",date);          \n        //f的使用  \n        System.out.printf(\"年-月-日格式：%tF%n\",date);  \n        //d的使用  \n        System.out.printf(\"月/日/年格式：%tD%n\",date);  \n        //r的使用  \n        System.out.printf(\"HH:MM:SS PM格式（12时制）：%tr%n\",date);  \n        //t的使用  \n        System.out.printf(\"HH:MM:SS格式（24时制）：%tT%n\",date);  \n        //R的使用  \n        System.out.printf(\"HH:MM格式（24时制）：%tR\",date);  \n        //b的使用，月份简称  \n        String str=String.format(Locale.US,\"英文月份简称：%tb\",date);       \n        System.out.println(str);\n        System.out.printf(\"本地月份简称：%tb%n\",date);  \n        //B的使用，月份全称  \n        str=String.format(Locale.US,\"英文月份全称：%tB\",date);  \n        System.out.println(str);  \n        System.out.printf(\"本地月份全称：%tB%n\",date);  \n        //a的使用，星期简称  \n        str=String.format(Locale.US,\"英文星期的简称：%ta\",date);  \n        System.out.println(str);  \n        //A的使用，星期全称  \n        System.out.printf(\"本地星期的简称：%tA%n\",date);  \n        //C的使用，年前两位  \n        System.out.printf(\"年的前两位数字（不足两位前面补0）：%tC%n\",date);  \n        //y的使用，年后两位  \n        System.out.printf(\"年的后两位数字（不足两位前面补0）：%ty%n\",date);  \n        //j的使用，一年的天数  \n        System.out.printf(\"一年中的天数（即年的第几天）：%tj%n\",date);  \n        //m的使用，月份  \n        System.out.printf(\"两位数字的月份（不足两位前面补0）：%tm%n\",date);  \n        //d的使用，日（二位，不够补零）  \n        System.out.printf(\"两位数字的日（不足两位前面补0）：%td%n\",date);  \n        //e的使用，日（一位不补零）  \n        System.out.printf(\"月份的日（前面不补0）：%te\",date);\n\n        // 如果你需要重复提供日期，那么利用这种方式来格式化它的每一部分就有点复杂了。因此，可以利用一个格式化字符串指出要被格式化的参数的索引。索引必须紧跟在%后面，而且必须以$结束。例如：\n        // 使用toString()显示日期和时间\n        System.out.printf(\"%1$s %2$tB %2$td, %2$tY\", \"Due date:\", date);\n\n        // 或者，你可以使用 < 标志。它表明先前被格式化的参数要被再次使用。例如：\n        System.out.printf(\"%s %tB %<te, %<tY\", \"Due date:\", date);\n    \n    }\n}"
        },
        {
          "label": "SimpleDateFormat",
          "language": "java",
          "value": "import  java.util.*;\nimport java.text.*;\n \npublic class DateDemo {\n    public static void main(String[] args) {\n\n        Date dNow = new Date( );\n        SimpleDateFormat ft = new SimpleDateFormat (\"yyyy-MM-dd hh:mm:ss\");\n\n        System.out.println(\"当前时间为: \" + ft.format(dNow));\n\n        ft = new SimpleDateFormat (\"yyyy-MM-dd\"); \n \n        String input = args.length == 0 ? \"1818-11-11\" : args[0]; \n    \n        System.out.print(input + \" Parses as \"); \n    \n        Date t; \n    \n        try { \n            t = ft.parse(input); \n            System.out.println(t); \n        } catch (ParseException e) { \n            System.out.println(\"Unparseable using \" + ft); \n        }\n\n    }\n}"
        },
        {
          "label": "Calendar",
          "language": "java",
          "value": "import  java.util.*;\nimport java.text.*;\n \npublic class CalendarDemo {\n    public static void main(String[] args) {\n\n        Calendar c1 = Calendar.getInstance();\n        // 调用：\n        public final void set(int year,int month,int date)\n        c1.set(2009, 6, 12);//把Calendar对象c1的年月日分别设这为：2009、6、12\n        // 利用字段类型设置\n        // 如果只设定某个字段，例如日期的值，则可以使用如下set方法：\n        public void set(int field,int value)\n        // 把 c1对象代表的日期设置为10号，其它所有的数值会被重新计算\n        c1.set(Calendar.DATE,10);\n        // 把c1对象代表的年份设置为2008年，其他的所有数值会被重新计算\n        c1.set(Calendar.YEAR,2008);\n        // 其他字段属性set的意义以此类推\n\n        // Add设置\n        Calendar c1 = Calendar.getInstance();\n        // 把c1对象的日期加上10，也就是c1也就表示为10天后的日期，其它所有的数值会被重新计算\n        c1.add(Calendar.DATE, 10);\n        // 把c1对象的日期减去10，也就是c1也就表示为10天前的日期，其它所有的数值会被重新计算\n        c1.add(Calendar.DATE, -10);\n        // 其他字段属性的add的意义以此类推\n        \n        // Calendar类对象信息的获得\n        Calendar c1 = Calendar.getInstance();\n        // 获得年份\n        int year = c1.get(Calendar.YEAR);\n        // 获得月份\n        int month = c1.get(Calendar.MONTH) + 1;\n        // 获得日期\n        int date = c1.get(Calendar.DATE);\n        // 获得小时\n        int hour = c1.get(Calendar.HOUR_OF_DAY);\n        // 获得分钟\n        int minute = c1.get(Calendar.MINUTE);\n        // 获得秒\n        int second = c1.get(Calendar.SECOND);\n        // 获得星期几（注意（这个与Date类是不同的）：1代表星期日、2代表星期1、3代表星期二，以此类推）\n        int day = c1.get(Calendar.DAY_OF_WEEK);\n\n    }\n}"
        }
      ],
      "id": "AbH3EmmW",
      "createdAt": 1729592837789,
      "updatedAt": 1729594147444
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "PoO2O3Rm",
      "tagsIds": [],
      "description": null,
      "name": "Enumeration",
      "content": [
        {
          "label": "子片段 1",
          "language": "java",
          "value": "Enumeration<String> contextParams = this.getServletConfig().getServletContext().getInitParameterNames();\nwhile (contextParams.hasMoreElements()) {\n    System.out.println(contextParams.nextElement());\n}"
        }
      ],
      "id": "H2H5bD0j",
      "createdAt": 1729686223232,
      "updatedAt": 1729686318683
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "PoO2O3Rm",
      "tagsIds": [],
      "description": null,
      "name": "file operation",
      "content": [
        {
          "label": "子片段 1",
          "language": "java",
          "value": ""
        }
      ],
      "id": "21KruGVN",
      "createdAt": 1729687924866,
      "updatedAt": 1729687930030
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "8qmQazwt",
      "tagsIds": [
        "7pTrIPxa"
      ],
      "description": null,
      "name": "sort",
      "content": [
        {
          "label": "子片段 1",
          "language": "plain_text",
          "value": "const iconDir = 'mdi-folder'\nconst iconFile = 'mdi-clipboard-text'\nconst classDir = 'bg-blue text-white'\nconst classFile = 'bg-grey-lighten-1 text-white'\n\nlet items = [\n    {\n        \"dir\": false,\n        \"modify\": 1729691508477,\n        \"name\": \"vk_swiftshader_icd.json\",\n        \"size\": 106,\n    },\n    { dir: true, name: 'dir1', modify: 1729691508177, size: 0, },\n    { dir: false, name: 'windows11.ios', modify: 1729691508476, size: 6768468768, },\n]\nconst sortBy = \"name\"\nconst sortRevert = false\nconst fileMae = false\nitems.sort((a, b) => {\n    // Sort by dir (true comes before false)\n    if (a.dir != b.dir) {\n        return fileMae ? a.dir - b.dir : b.dir - a.dir;\n    }\n    // Sort by name if dir is the same\n    return sortRevert ? b[sortBy].localeCompare(a.name) : a[sortBy].localeCompare(b.name);\n});"
        }
      ],
      "id": "PFkiSRvG",
      "createdAt": 1729698448452,
      "updatedAt": 1729698459429
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "twJ-w0RU",
      "tagsIds": [],
      "description": null,
      "name": "adb",
      "content": [
        {
          "label": "子片段 1",
          "language": "kotlin",
          "value": ""
        }
      ],
      "id": "c3yFPa0K",
      "createdAt": 1729995684313,
      "updatedAt": 1729995689739
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "twJ-w0RU",
      "tagsIds": [],
      "description": null,
      "name": "adb shell",
      "content": [
        {
          "label": "子片段 1",
          "language": "sh",
          "value": "cat /sys/class/thermal/thermal_zone0/temp\n=> 31000\n(31 degress)"
        }
      ],
      "id": "7R69n-CJ",
      "createdAt": 1729995696227,
      "updatedAt": 1729995763766
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "mdpwqphi",
      "tagsIds": [],
      "description": null,
      "name": "index",
      "content": [
        {
          "label": "unique index",
          "language": "mysql",
          "value": "-- create unique index\nCREATE UNIQUE INDEX [index_name] ON [table_name]([col_name]);\n\n-- drop a index\nDROP INDEX [index_name];\n\n-- create unique constraint will auto create unique index\nALTER TABLE [table_name] ADD CONSTRAINT [constraint_name] UNIQUE ([col_name]);"
        },
        {
          "label": "子片段 2",
          "language": "mysql",
          "value": ""
        }
      ],
      "id": "wNyYssz6",
      "createdAt": 1730010827372,
      "updatedAt": 1730011205993
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "PoO2O3Rm",
      "tagsIds": [],
      "description": null,
      "name": "JDBCUtil",
      "content": [
        {
          "label": "子片段 1",
          "language": "java",
          "value": "import com.zaxxer.hikari.HikariConfig;\nimport com.zaxxer.hikari.HikariDataSource;\n\nimport javax.sql.DataSource;\nimport java.io.InputStream;\nimport java.sql.Connection;\nimport java.sql.PreparedStatement;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.util.Date;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Properties;\n\n/**\n * JDBC Util class\n * 1. connection pool maintenance\n * 2. get connection\n * 3. recycle connection\n * 4. bind one connectino to one thread\n */\npublic class JDBCUtil {\n    private static volatile DataSource ds;\n    private static ThreadLocal<Connection> threadConn = new ThreadLocal<>();\n\n    private static void genDataSource() {\n        try {\n            Properties prop = new Properties();\n            InputStream in = JDBCUtil.class.getClassLoader().getResourceAsStream(\"db_h.properties\");\n            prop.load(in);\n            HikariConfig config = new HikariConfig(prop);\n            ds = new HikariDataSource(config);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    public static synchronized DataSource getDataSource() {\n        if (ds == null) {\n            System.out.println(\"construct DataSource\");\n            genDataSource();\n        }\n        return ds;\n    }\n\n    public static Connection getConnection() throws SQLException {\n        Connection conn = threadConn.get();\n        if (conn == null) {\n            conn = JDBCUtilWithThreadLocal.getDataSource().getConnection();\n            threadConn.set(conn);\n        }\n        return conn;\n    }\n\n    public static void release() throws SQLException {\n        Connection conn = threadConn.get();\n        if (conn != null && conn.getAutoCommit()) {\n            threadConn.remove();\n            conn.close();\n        } else {\n            System.out.println(\"change connection autocommit to true before release\");\n        }\n    }\n}\n"
        }
      ],
      "id": "_OfPgq3j",
      "createdAt": 1730442665245,
      "updatedAt": 1730443741203
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "PoO2O3Rm",
      "tagsIds": [],
      "description": null,
      "name": "Hikari",
      "content": [
        {
          "label": "Usage",
          "language": "java",
          "value": "package com.li.schedule.sqlTest.advanced;\n\nimport com.li.schedule.sqlTest.advanced.pojo.Employee;\nimport com.zaxxer.hikari.HikariConfig;\nimport com.zaxxer.hikari.HikariDataSource;\nimport org.junit.jupiter.api.Test;\n\nimport javax.sql.DataSource;\nimport java.io.InputStream;\nimport java.sql.Connection;\nimport java.sql.PreparedStatement;\nimport java.sql.ResultSet;\nimport java.util.Date;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Properties;\n\npublic class HikariTest {\n    @Test\n    public void hardTest() throws Exception {\n        // create hikari datasource and configure\n        HikariDataSource hds = new HikariDataSource();\n        hds.setDriverClassName(\"com.mysql.cj.jdbc.Driver\");\n        hds.setJdbcUrl(\"jdbc:mysql://127.0.0.1:13306/webtest\");\n        hds.setUsername(\"root\");\n        hds.setPassword(\"lijunjie\");\n        hds.setMinimumIdle(5);\n        hds.setMaximumPoolSize(10);\n\n        System.out.println(hds);\n\n        List<Thread> threads = new LinkedList<>();\n        for (int i = 1; i < 12; i++) {\n            final String emp_id = String.valueOf(i);\n            threads.add(new Thread(() -> {\n                System.out.println(Thread.currentThread().getId());\n                try (Connection conn = hds.getConnection()) {\n                    System.out.println(conn);\n                    Thread.sleep(2000);\n                    try (PreparedStatement ps = conn.prepareStatement(\"SELECT * FROM `t_emp` WHERE `emp_id`=?\")) {\n                        ps.setString(1, emp_id);\n                        try (ResultSet rs = ps.executeQuery()) {\n                            while (rs.next())\n                                System.out.println(\n                                        new Employee(\n                                                rs.getInt(\"emp_id\"),\n                                                rs.getString(\"emp_name\"),\n                                                rs.getInt(\"emp_age\"),\n                                                rs.getDouble(\"emp_salary\")\n                                        )\n                                );\n                        }\n                    }\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }));\n        }\n        for (Thread thread : threads)\n            thread.start();\n\n        for (Thread thread : threads)\n            thread.join();\n\n    }\n\n    @Test\n    public void softTest() throws Exception {\n        // create properties set\n        Properties hikariProperties = new Properties();\n        // read properties file\n        InputStream propertiesInputStream = this.getClass().getClassLoader().getResourceAsStream(\"db_h.properties\");\n        hikariProperties.load(propertiesInputStream);\n        // create HikariConfig object with properties set\n        HikariConfig hikariConfig = new HikariConfig(hikariProperties);\n        // create HikariDataSource with HikariConfig\n        HikariDataSource hds = new HikariDataSource(hikariConfig);\n\n        System.out.println(hds);\n\n        Date start = new Date();\n\n        List<Thread> threads = new LinkedList<>();\n        for (int i = 1; i < 12; i++) {\n            final String emp_id = String.valueOf(i);\n            threads.add(new Thread(() -> {\n                System.out.println(Thread.currentThread().getId());\n                try (Connection conn = hds.getConnection()) {\n                    System.out.println(conn);\n                    Thread.sleep(2000);\n                    try (PreparedStatement ps = conn.prepareStatement(\"SELECT * FROM `t_emp` WHERE `emp_id`=?\")) {\n                        ps.setString(1, emp_id);\n                        try (ResultSet rs = ps.executeQuery()) {\n                            while (rs.next())\n                                System.out.println(\n                                        new Employee(\n                                                rs.getInt(\"emp_id\"),\n                                                rs.getString(\"emp_name\"),\n                                                rs.getInt(\"emp_age\"),\n                                                rs.getDouble(\"emp_salary\")\n                                        )\n                                );\n                        }\n                    }\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }));\n        }\n        for (Thread thread : threads)\n            thread.start();\n\n        for (Thread thread : threads)\n            thread.join();\n\n        System.out.println(String.format(\"use time: %dms\", System.currentTimeMillis() - start.getTime()));\n    }\n}\n"
        },
        {
          "label": "properties",
          "language": "properties",
          "value": "driverClassName=com.mysql.cj.jdbc.Driver\njdbcUrl=jdbc:mysql://127.0.0.1:13306/webtest\nusername=root\npassword=lijunjie\nminimumIdle=5\nmaximumPoolSize=10"
        }
      ],
      "id": "Hk937jtY",
      "createdAt": 1730442723263,
      "updatedAt": 1730442804959
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "PoO2O3Rm",
      "tagsIds": [],
      "description": null,
      "name": "Druid",
      "content": [
        {
          "label": "Usage",
          "language": "java",
          "value": "package com.li.schedule.sqlTest.advanced;\n\nimport com.alibaba.druid.pool.DruidDataSource;\nimport com.alibaba.druid.pool.DruidDataSourceFactory;\nimport com.li.schedule.sqlTest.advanced.pojo.Employee;\nimport org.junit.jupiter.api.Test;\n\nimport javax.sql.DataSource;\nimport java.io.InputStream;\nimport java.sql.Connection;\nimport java.sql.PreparedStatement;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.util.Date;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Properties;\n\npublic class DruidTest {\n\n    @Test\n    public void hardDruid() throws Exception {\n        DruidDataSource dds = new DruidDataSource();\n        dds.setDriverClassName(\"com.mysql.cj.jdbc.Driver\");\n        dds.setUsername(\"root\");\n        dds.setPassword(\"lijunjie\");\n        dds.setUrl(\"jdbc:mysql://127.0.0.1:13306/webtest\");\n        dds.setInitialSize(5);\n        dds.setMaxActive(10);\n\n        List<Thread> threads = new LinkedList<>();\n        for (int i = 1; i < 12; i++) {\n            final String emp_id = String.valueOf(i);\n            threads.add(new Thread(() -> {\n                System.out.println(Thread.currentThread().getId());\n                try (Connection conn = dds.getConnection()) {\n                    System.out.println(conn);\n                    Thread.sleep(2000);\n                    try (PreparedStatement ps = conn.prepareStatement(\"SELECT * FROM `t_emp` WHERE `emp_id`=?\")) {\n                        ps.setString(1, emp_id);\n                        try (ResultSet rs = ps.executeQuery()) {\n                            while (rs.next())\n                                System.out.println(\n                                        new Employee(\n                                                rs.getInt(\"emp_id\"),\n                                                rs.getString(\"emp_name\"),\n                                                rs.getInt(\"emp_age\"),\n                                                rs.getDouble(\"emp_salary\")\n                                        )\n                                );\n                        }\n                    }\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }));\n        }\n        for (Thread thread : threads)\n            thread.start();\n\n        for (Thread thread : threads)\n            thread.join();\n\n    }\n\n    @Test\n    public void softDruid() throws Exception {\n        // get properties context\n        Properties druidProperties = new Properties();\n        InputStream propertiesInputStream = this.getClass().getClassLoader().getResourceAsStream(\"db.properties\");\n        druidProperties.load(propertiesInputStream);\n\n        // create DruidDataSource by DruidDataSourceFactory\n        DataSource dds = DruidDataSourceFactory.createDataSource(druidProperties);\n        System.out.println(dds);\n\n        Date start = new Date();\n\n        List<Thread> threads = new LinkedList<>();\n        for (int i = 1; i < 12; i++) {\n            final String emp_id = String.valueOf(i);\n            threads.add(new Thread(() -> {\n                System.out.println(Thread.currentThread().getId());\n                try (Connection conn = dds.getConnection()) {\n                    System.out.println(conn);\n                    Thread.sleep(2000);\n                    try (PreparedStatement ps = conn.prepareStatement(\"SELECT * FROM `t_emp` WHERE `emp_id`=?\")) {\n                        ps.setString(1, emp_id);\n                        try (ResultSet rs = ps.executeQuery()) {\n                            while (rs.next())\n                                System.out.println(\n                                        new Employee(\n                                                rs.getInt(\"emp_id\"),\n                                                rs.getString(\"emp_name\"),\n                                                rs.getInt(\"emp_age\"),\n                                                rs.getDouble(\"emp_salary\")\n                                        )\n                                );\n                        }\n                    }\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }));\n        }\n        for (Thread thread : threads)\n            thread.start();\n\n        for (Thread thread : threads)\n            thread.join();\n\n        System.out.println(String.format(\"use time: %dms\", System.currentTimeMillis() - start.getTime()));\n    }\n}\n"
        },
        {
          "label": "properties",
          "language": "properties",
          "value": "driverClassName=com.mysql.cj.jdbc.Driver\nurl=jdbc:mysql://127.0.0.1:13306/webtest\nusername=root\npassword=lijunjie\ninitialSize=5\nmaxAcitve=10"
        }
      ],
      "id": "Y92k6ez5",
      "createdAt": 1730442729363,
      "updatedAt": 1730443055717
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "PoO2O3Rm",
      "tagsIds": [],
      "description": null,
      "name": "BaseDao",
      "content": [
        {
          "label": "子片段 1",
          "language": "java",
          "value": "package com.li.schedule.dao.impl;\n\nimport com.li.schedule.utils.JDBCUtil;\n\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\n\nimport java.lang.reflect.Field;\nimport java.sql.*;\nimport java.util.LinkedList;\nimport java.util.List;\n\npublic class BaseDao {\n    /**\n     * general update one row\n     *\n     * @param sql:    sql setence\n     * @param params: variable in sql\n     * @return affect row\n     * @throws Exception\n     */\n    public int executeUpdate(String sql, Object... params) throws Exception {\n        Connection conn = JDBCUtil.getConnection();\n        System.out.println(String.format(\"[Connection]: %s, [autoCommit]: %s\" , conn, conn.getAutoCommit()));\n        int row = 0;\n        try (PreparedStatement ps = conn.prepareStatement(sql)) {\n            for (int i = 0; i < params.length; i++) {\n                ps.setObject(i + 1, params[i]);\n            }\n            row = ps.executeUpdate();\n        }\n        JDBCUtil.release();\n        return row;\n    }\n\n    /**\n     * general insert one row with generated keys return\n     *\n     * @param sql:    sql setence\n     * @param params: variable in sql\n     * @return affect row\n     * @throws Exception\n     */\n    public int executeInsertWithGenKey(String sql, Object... params) throws Exception {\n        Connection conn = JDBCUtil.getConnection();\n        int GENERATED_KEY = -1;\n        Timestamp TIMESTAMP = null;\n        try (PreparedStatement ps = conn.prepareStatement(sql, Statement.RETURN_GENERATED_KEYS)) {\n            for (int i = 0; i < params.length; i++) {\n                ps.setObject(i + 1, params[i]);\n            }\n            int row = ps.executeUpdate();\n            if (row > 0) {\n                try (ResultSet rs = ps.getGeneratedKeys()) {\n                    if (rs.next()) {\n                        GENERATED_KEY = rs.getInt(1);\n                    }\n                }\n            }\n        }\n        JDBCUtil.release();\n        return GENERATED_KEY;\n    }\n\n    /**\n     * general query\n     *\n     * @param clazz:  DAO object class\n     * @param sql:    sql setence\n     * @param params: variable in sql\n     * @return query result\n     * @throws\n     */\n    public <T> List<T> executeQuery(Class<T> clazz, String sql, Object... params) throws Exception {\n        Connection conn = JDBCUtil.getConnection();\n        List<T> clazzList = new LinkedList<>();\n        try (PreparedStatement ps = conn.prepareStatement(sql)) {\n            for (int i = 0; i < params.length; i++) {\n                ps.setObject(i + 1, params[i]);\n            }\n            try (ResultSet rs = ps.executeQuery()) {\n                ResultSetMetaData rsmd = rs.getMetaData();\n                int columnCount = rsmd.getColumnCount();\n                while (rs.next()) {\n                    T t = clazz.getDeclaredConstructor().newInstance();\n                    for (int i = 1; i <= columnCount; i++) {\n                        String fieldName = rsmd.getColumnLabel(i);\n                        Field field = clazz.getDeclaredField(fieldName);\n                        field.setAccessible(true);\n                        field.set(t, rs.getObject(i));\n                        field.setAccessible(false);\n                    }\n                    clazzList.add(t);\n                }\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n        JDBCUtil.release();\n        return clazzList;\n    }\n\n    /**\n     * general only first row of query\n     *\n     * @param clazz:  DAO object class\n     * @param sql:    sql setence\n     * @param params: variable in sql\n     * @return query result\n     * @throws\n     */\n    public <T> T executeQuery4OneRow(Class<T> clazz, String sql, Object... params) throws Exception {\n        List<T> result = this.executeQuery(clazz, sql, params);\n        if (result.size() > 0) {\n            return result.get(0);\n        } else return null;\n    }\n\n    public static String MD5(String input) {\n        try {\n            MessageDigest md = MessageDigest.getInstance(\"MD5\");\n            byte[] messageDigest = md.digest(input.getBytes());\n            // Convert byte array into signum representation\n            StringBuilder sb = new StringBuilder();\n            for (byte b : messageDigest) {\n                sb.append(String.format(\"%02x\", b));\n            }\n            return sb.toString();\n        } catch (NoSuchAlgorithmException e) {\n            throw new RuntimeException(e);\n        }\n    }\n}\n"
        }
      ],
      "id": "MHNAR16x",
      "createdAt": 1730443704269,
      "updatedAt": 1730443781611
    },
    {
      "isDeleted": false,
      "isFavorites": false,
      "folderId": "DvcfGvIx",
      "tagsIds": [],
      "description": null,
      "name": "YOLOV8",
      "content": [
        {
          "label": "TaskAlignedAssigner",
          "language": "python",
          "value": "# Ultralytics YOLO 🚀, AGPL-3.0 license\n\nimport torch\nimport torch.nn as nn\n\nfrom .checks import check_version\nfrom .metrics import bbox_iou, probiou\nfrom .ops import xywhr2xyxyxyxy\n\nTORCH_1_10 = check_version(torch.__version__, \"1.10.0\")\n\n\nclass TaskAlignedAssigner(nn.Module):\n    \"\"\"\n    A task-aligned assigner for object detection.\n\n    This class assigns ground-truth (gt) objects to anchors based on the task-aligned metric, which combines both\n    classification and localization information.\n\n    Attributes:\n        topk (int): The number of top candidates to consider.\n        num_classes (int): The number of object classes.\n        alpha (float): The alpha parameter for the classification component of the task-aligned metric.\n        beta (float): The beta parameter for the localization component of the task-aligned metric.\n        eps (float): A small value to prevent division by zero.\n    \"\"\"\n\n    def __init__(self, topk=13, num_classes=80, alpha=1.0, beta=6.0, eps=1e-9):\n        \"\"\"Initialize a TaskAlignedAssigner object with customizable hyperparameters.\"\"\"\n        super().__init__()\n        self.topk = topk\n        self.num_classes = num_classes\n        self.bg_idx = num_classes # 背景的类别编号\n        self.alpha = alpha\n        self.beta = beta\n        self.eps = eps\n\n    @torch.no_grad()\n    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n        \"\"\"\n        Compute the task-aligned assignment. Reference code is available at\n        https://github.com/Nioolek/PPYOLOE_pytorch/blob/master/ppyoloe/assigner/tal_assigner.py.\n\n        Args:\n            pd_scores (Tensor): shape(bs, num_total_anchors, num_classes)，预测分类，锚点属于每一个分类的概率。\n            pd_bboxes (Tensor): shape(bs, num_total_anchors, 4)，预测偏移量，已经从 DFL 的一般分布转为标量（Scalar）。 \n            anc_points (Tensor): shape(num_total_anchors, 2)，辅助 Tensor，方便锚点偏移量和框坐标的转换。\n            gt_labels (Tensor): shape(bs, n_max_boxes, 1)，人工标记的 GT 框对应的分类编号\n            gt_bboxes (Tensor): shape(bs, n_max_boxes, 4)，人工标记的 GT 框坐标，xyxy\n            mask_gt (Tensor): shape(bs, n_max_boxes, 1)，由于 GT 框数量一般少于 n_max_boxes, 向量运算一般基于同一形状，该变量表示 gt_bboxes 的位置是否有真实 GT 还是空位补齐。（待确认）\n\n        Returns:\n            target_labels (Tensor): shape(bs, num_total_anchors)\n            target_bboxes (Tensor): shape(bs, num_total_anchors, 4)\n            target_scores (Tensor): shape(bs, num_total_anchors, num_classes)\n            fg_mask (Tensor): shape(bs, num_total_anchors)\n            target_gt_idx (Tensor): shape(bs, num_total_anchors)\n        \"\"\"\n        self.bs = pd_scores.shape[0]\n        self.n_max_boxes = gt_bboxes.shape[1]\n\n        if self.n_max_boxes == 0:\n            device = gt_bboxes.device\n            return (\n                torch.full_like(pd_scores[..., 0], self.bg_idx).to(device),\n                torch.zeros_like(pd_bboxes).to(device),\n                torch.zeros_like(pd_scores).to(device),\n                torch.zeros_like(pd_scores[..., 0]).to(device),\n                torch.zeros_like(pd_scores[..., 0]).to(device),\n            )\n\n        mask_pos, align_metric, overlaps = self.get_pos_mask(\n            pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt\n        )\n\n        target_gt_idx, fg_mask, mask_pos = self.select_highest_overlaps(mask_pos, overlaps, self.n_max_boxes)\n\n        # Assigned target\n        target_labels, target_bboxes, target_scores = self.get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask)\n\n        # Normalize\n        align_metric *= mask_pos\n        pos_align_metrics = align_metric.amax(dim=-1, keepdim=True)  # b, max_num_obj\n        pos_overlaps = (overlaps * mask_pos).amax(dim=-1, keepdim=True)  # b, max_num_obj\n        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n        target_scores = target_scores * norm_align_metric\n\n        return target_labels, target_bboxes, target_scores, fg_mask.bool(), target_gt_idx\n\n    def get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n        # 初步筛选：对每一个 GT 框，生成掩码锚点图（也就是说，获取锚点是否在任一 GT 内的掩码，不涉及预测值）。\n        # 返回 shape(b, max_num_obj, h*w)\n        \"\"\"Get in_gts mask, (b, max_num_obj, h*w).\"\"\"\n        mask_in_gts = self.select_candidates_in_gts(anc_points, gt_bboxes)\n        # 获取锚点预测值和 GT 框的对齐分数及 IOU 值。\n        # 返回 shape(b, max_num_obj, h*w)\n        # Get anchor_align metric, (b, max_num_obj, h*w)\n        align_metric, overlaps = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts * mask_gt)\n        # Get topk_metric mask, (b, max_num_obj, h*w)\n        mask_topk = self.select_topk_candidates(align_metric, topk_mask=mask_gt.expand(-1, -1, self.topk).bool())\n        # Merge all mask to a final mask, (b, max_num_obj, h*w)\n        mask_pos = mask_topk * mask_in_gts * mask_gt\n\n        return mask_pos, align_metric, overlaps\n\n    def get_box_metrics(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt):\n        \"\"\"Compute alignment metric given predicted and ground truth bounding boxes.\n        （计算所有预测框和 GT 框的对齐分数、CIoU值，已经通过锚点在 GT 框内过滤）\n        \n        Args:\n            pd_scores (Tensor): shape(bs, num_total_anchors, num_classes)，预测分类，锚点属于每一个分类的概率。\n        \"\"\"\n        na = pd_bboxes.shape[-2] # num_total_anchors，锚点数量\n        mask_gt = mask_gt.bool()  # (b, max_num_obj, h*w)\n        overlaps = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\n\n        # 初始化为 0。 bbox_scores (Tensor): shape(bs, n_max_boxes, na)，注意，形状和 pd_scores 不相同。\n        bbox_scores = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\n\n        ind = torch.zeros([2, self.bs, self.n_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\n        # 【方便理解代码的示例】\n        #   代码：torch.arange(end=3).view(-1, 1).expand(-1, 4)\n        #   返回：tensor([[0, 0, 0, 0], \n        #                [1, 1, 1, 1], \n        #                [2, 2, 2, 2]])\n        # 返回 (b, max_num_obj)\n        ind[0] = torch.arange(end=self.bs).view(-1, 1).expand(-1, self.n_max_boxes)  \n        # gt_labels (Tensor): shape(bs, n_max_boxes, 1)。对最后一个维度挤压后，返回 (b, max_num_obj)\n        ind[1] = gt_labels.squeeze(-1)  \n        \n        # Get the scores of each grid for each gt cls\n        bbox_scores[mask_gt] = pd_scores[ind[0], :, ind[1]][mask_gt]  # b, max_num_obj, h*w\n\n        # pd_bboxes.unsqueeze(1): unsqueeze 操作用于增加一个维度。这里在索引为1的位置增加一个维度，使得 pd_bboxes 的形状从 (bs, h*w, 4) 变为 (bs, 1, h*w, 4)。\n        #           .expand(-1, self.n_max_boxes, -1, -1): expand 操作用于扩展张量的形状。参数 -1 表示该维度保持原样不变。因此，这个操作将 (bs, 1, h*w, 4) 形状的张量扩展为 (bs, n_max_boxes, h*w, 4)。\n        #           [mask_gt]: 最后，使用 mask_gt 张量作为索引来选择 expand 后张量中对应位置的元素。mask_gt 是一个布尔型张量，形状为 (bs, n_max_boxes, h*w)，其中 True 表示相应的锚点与某个真实目标匹配，False 表示不匹配。这个索引操作将 (bs, n_max_boxes, h*w, 4) 形状的张量中不匹配的位置（即 False 位置）设置为0（因为在前面创建的 overlaps 和 bbox_scores 中使用了 torch.zeros），而匹配的位置则保留了扩展后的值。\n        # (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\n\t\tpd_boxes = pd_bboxes.unsqueeze(1).expand(-1, self.n_max_boxes, -1, -1)[mask_gt]\n        # 扩展后和 pd_boxes 形状一致。\n        gt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[mask_gt]\n        # 计算预测框和 GT 框的 iou 得分\n        overlaps[mask_gt] = self.iou_calculation(gt_boxes, pd_boxes)\n        # 最终的对齐分数。参考论文 2108.07755 - 3.2.1 Task-aligned Sample Assignment。\n        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n        # 每个 GT 框在锚点图上的的对齐得分、重叠度， shape(b, max_num_obj, na)\n        return align_metric, overlaps\n\n    def iou_calculation(self, gt_bboxes, pd_bboxes):\n        \"\"\"IoU calculation for horizontal bounding boxes.\"\"\"\n        return bbox_iou(gt_bboxes, pd_bboxes, xywh=False, CIoU=True).squeeze(-1).clamp_(0)\n\n    def select_topk_candidates(self, metrics, largest=True, topk_mask=None):\n        \"\"\"\n        Select the top-k candidates based on the given metrics.（对每个 GT 框，根据对齐分数，选取 topk 个锚点）\n\n        Args:\n            metrics (Tensor): A tensor of shape (b, max_num_obj, h*w), where b is the batch size,\n                              max_num_obj is the maximum number of objects, and h*w represents the\n                              total number of anchor points.\n            largest (bool): If True, select the largest values; otherwise, select the smallest values.\n            topk_mask (Tensor): An optional boolean tensor of shape (b, max_num_obj, topk), where\n                                topk is the number of top candidates to consider. If not provided,\n                                the top-k values are automatically computed based on the given metrics.\n\n        Returns:\n            (Tensor): A tensor of shape (b, max_num_obj, h*w) containing the selected top-k candidates.\n        \"\"\"\n        # (b, max_num_obj, topk)\n        topk_metrics, topk_idxs = torch.topk(metrics, self.topk, dim=-1, largest=largest)\n        if topk_mask is None:\n            topk_mask = (topk_metrics.max(-1, keepdim=True)[0] > self.eps).expand_as(topk_idxs)\n        # (b, max_num_obj, topk)\n        topk_idxs.masked_fill_(~topk_mask, 0)\n\n        # (b, max_num_obj, topk, h*w) -> (b, max_num_obj, h*w)\n        count_tensor = torch.zeros(metrics.shape, dtype=torch.int8, device=topk_idxs.device)\n        ones = torch.ones_like(topk_idxs[:, :, :1], dtype=torch.int8, device=topk_idxs.device)\n        for k in range(self.topk):\n            # Expand topk_idxs for each value of k and add 1 at the specified positions\n            count_tensor.scatter_add_(-1, topk_idxs[:, :, k : k + 1], ones)\n        # count_tensor.scatter_add_(-1, topk_idxs, torch.ones_like(topk_idxs, dtype=torch.int8, device=topk_idxs.device))\n        # Filter invalid bboxes\n        count_tensor.masked_fill_(count_tensor > 1, 0)\n\n        return count_tensor.to(metrics.dtype)\n\n    def get_targets(self, gt_labels, gt_bboxes, target_gt_idx, fg_mask):\n        \"\"\"\n        Compute target labels, target bounding boxes, and target scores for the positive anchor points.\n\n        Args:\n            gt_labels (Tensor): Ground truth labels of shape (b, max_num_obj, 1), where b is the\n                                batch size and max_num_obj is the maximum number of objects.\n            gt_bboxes (Tensor): Ground truth bounding boxes of shape (b, max_num_obj, 4).\n            target_gt_idx (Tensor): Indices of the assigned ground truth objects for positive\n                                    anchor points, with shape (b, h*w), where h*w is the total\n                                    number of anchor points.\n            fg_mask (Tensor): A boolean tensor of shape (b, h*w) indicating the positive\n                              (foreground) anchor points.\n\n        Returns:\n            (Tuple[Tensor, Tensor, Tensor]): A tuple containing the following tensors:\n                - target_labels (Tensor): Shape (b, h*w), containing the target labels for\n                                          positive anchor points.\n                - target_bboxes (Tensor): Shape (b, h*w, 4), containing the target bounding boxes\n                                          for positive anchor points.\n                - target_scores (Tensor): Shape (b, h*w, num_classes), containing the target scores\n                                          for positive anchor points, where num_classes is the number\n                                          of object classes.\n        \"\"\"\n        # Assigned target labels, (b, 1)\n        batch_ind = torch.arange(end=self.bs, dtype=torch.int64, device=gt_labels.device)[..., None]\n        target_gt_idx = target_gt_idx + batch_ind * self.n_max_boxes  # (b, h*w)\n        target_labels = gt_labels.long().flatten()[target_gt_idx]  # (b, h*w)\n\n        # Assigned target boxes, (b, max_num_obj, 4) -> (b, h*w, 4)\n        target_bboxes = gt_bboxes.view(-1, gt_bboxes.shape[-1])[target_gt_idx]\n\n        # Assigned target scores\n        target_labels.clamp_(0)\n\n        # 10x faster than F.one_hot()\n        target_scores = torch.zeros(\n            (target_labels.shape[0], target_labels.shape[1], self.num_classes),\n            dtype=torch.int64,\n            device=target_labels.device,\n        )  # (b, h*w, 80)\n        target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n\n        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.num_classes)  # (b, h*w, 80)\n        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n\n        return target_labels, target_bboxes, target_scores\n\n    @staticmethod\n    def select_candidates_in_gts(xy_centers, gt_bboxes, eps=1e-9):\n        \"\"\"\n        Select positive anchor centers within ground truth bounding boxes.\n\t\t选择在 GT 内的锚点中心，用作正样本。即对每一个 GT 框，生成掩码锚点图。\n\n        Args:\n            xy_centers (torch.Tensor): Anchor center coordinates, shape (h*w, 2).，锚点中心坐标，不涉及模型预测值\n            gt_bboxes (torch.Tensor): Ground truth bounding boxes, shape (b, n_boxes, 4).\n            eps (float, optional): Small value for numerical stability. Defaults to 1e-9.\n\n        Returns:\n            (torch.Tensor): Boolean mask of positive anchors, shape (b, n_boxes, h*w).\n\n        Note:\n            b: batch size, n_boxes: number of ground truth boxes, h: height, w: width.\n            Bounding box format: [x_min, y_min, x_max, y_max].\n        \"\"\"\n        n_anchors = xy_centers.shape[0] # 锚点总数\n        bs, n_boxes, _ = gt_bboxes.shape # 获取批次大小bs和每个图片中真实目标的数量 n_boxes\n\n        # 将 gt_bboxes 拆分为左上角坐标 lt 和右下角坐标 rb\n        # lt (Tensor): shape(b*n_boxes, 1, 2)\n        lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n    \n        # 计算了每个锚点中心相对于每个 GT 左上角的差值以及每个 GT 右下角与锚点中心的差值，小于 0 表示不在框内。\n        # xy_centers[None] (Tensor):shape(1, h*w, 2)\n        # xy_centers[None] - lt (Tensor): shape(b*n_boxes, h*w, 2)，这里用到了第 0、1 维度的自动扩展（即广播）。\n        # bbox_deltas (Tensor): shape(b, n_boxes, n_anchors, 4)，\n        bbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2).view(bs, n_boxes, n_anchors, -1)\n        # return (bbox_deltas.min(3)[0] > eps).to(gt_bboxes.dtype)\n\t\t# amin(3)要求所有 4 个比对值都必须大于 0\n        return bbox_deltas.amin(3).gt_(eps)\n\n    @staticmethod\n    def select_highest_overlaps(mask_pos, overlaps, n_max_boxes):\n        \"\"\"\n        Select anchor boxes with highest IoU when assigned to multiple ground truths.\n\n        Args:\n            mask_pos (torch.Tensor): Positive mask, shape (b, n_max_boxes, h*w).\n            overlaps (torch.Tensor): IoU overlaps, shape (b, n_max_boxes, h*w).\n            n_max_boxes (int): Maximum number of ground truth boxes.\n\n        Returns:\n            target_gt_idx (torch.Tensor): Indices of assigned ground truths, shape (b, h*w).\n            fg_mask (torch.Tensor): Foreground mask, shape (b, h*w).\n            mask_pos (torch.Tensor): Updated positive mask, shape (b, n_max_boxes, h*w).\n\n        Note:\n            b: batch size, h: height, w: width.\n        \"\"\"\n        # Convert (b, n_max_boxes, h*w) -> (b, h*w)\n        fg_mask = mask_pos.sum(-2) # 对每一个锚点，统计对应的 GT 总数\n        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).expand(-1, n_max_boxes, -1)  # (b, n_max_boxes, h*w)\n            max_overlaps_idx = overlaps.argmax(1)  # (b, h*w)\n\n            is_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\n            is_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)\n\n            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()  # (b, n_max_boxes, h*w)\n            fg_mask = mask_pos.sum(-2)\n        # Find each grid serve which gt(index)\n        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n        return target_gt_idx, fg_mask, mask_pos\n\n\nclass RotatedTaskAlignedAssigner(TaskAlignedAssigner):\n    \"\"\"Assigns ground-truth objects to rotated bounding boxes using a task-aligned metric.\"\"\"\n\n    def iou_calculation(self, gt_bboxes, pd_bboxes):\n        \"\"\"IoU calculation for rotated bounding boxes.\"\"\"\n        return probiou(gt_bboxes, pd_bboxes).squeeze(-1).clamp_(0)\n\n    @staticmethod\n    def select_candidates_in_gts(xy_centers, gt_bboxes):\n        \"\"\"\n        Select the positive anchor center in gt for rotated bounding boxes.\n\n        Args:\n            xy_centers (Tensor): shape(h*w, 2)\n            gt_bboxes (Tensor): shape(b, n_boxes, 5)\n\n        Returns:\n            (Tensor): shape(b, n_boxes, h*w)\n        \"\"\"\n        # (b, n_boxes, 5) --> (b, n_boxes, 4, 2)\n        corners = xywhr2xyxyxyxy(gt_bboxes)\n        # (b, n_boxes, 1, 2)\n        a, b, _, d = corners.split(1, dim=-2)\n        ab = b - a\n        ad = d - a\n\n        # (b, n_boxes, h*w, 2)\n        ap = xy_centers - a\n        norm_ab = (ab * ab).sum(dim=-1)\n        norm_ad = (ad * ad).sum(dim=-1)\n        ap_dot_ab = (ap * ab).sum(dim=-1)\n        ap_dot_ad = (ap * ad).sum(dim=-1)\n        return (ap_dot_ab >= 0) & (ap_dot_ab <= norm_ab) & (ap_dot_ad >= 0) & (ap_dot_ad <= norm_ad)  # is_in_box\n\n\ndef make_anchors(feats, strides, grid_cell_offset=0.5):\n    \"\"\"Generate anchors from features.\n        feats 为模型预测值，对于 Detect 任务，head.py 中 Detect 的输出为(x0,x1,x2)，x[0]的shape(N, reg_max*4+nc , H, W)\n        输出 anchor_points 为坐标点的集合，包括（x0,x1,x2）H,W 对应的所有坐标。\n    \"\"\"\n    anchor_points, stride_tensor = [], []\n    assert feats is not None\n    dtype, device = feats[0].dtype, feats[0].device\n    for i, stride in enumerate(strides):\n        _, _, h, w = feats[i].shape\n        sx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset  # shift x\n        sy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset  # shift y\n        sy, sx = torch.meshgrid(sy, sx, indexing=\"ij\") if TORCH_1_10 else torch.meshgrid(sy, sx)\n        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n    return torch.cat(anchor_points), torch.cat(stride_tensor)\n\n\ndef dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n    lt, rb = distance.chunk(2, dim)\n    x1y1 = anchor_points - lt\n    x2y2 = anchor_points + rb\n    if xywh:\n        c_xy = (x1y1 + x2y2) / 2\n        wh = x2y2 - x1y1\n        return torch.cat((c_xy, wh), dim)  # xywh bbox\n    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n\n\ndef bbox2dist(anchor_points, bbox, reg_max):\n    \"\"\"Transform bbox(xyxy) to dist(ltrb).\n    将框由 xyxy 坐标形式变成相对锚点的偏移量形式（左上右下）。\n    anchor_points 为坐标点，如对于长宽为 8*5 的图，\n    [[0,0],[1,0], ...,  [7,0], \n     [0,1], [1,1], ..., [7,1],\n     [0,2], [1,2], ..., [7,2],\n     [0,3], [1,3], ..., [7,3],\n     [0,4], [1,4], ..., [7,4]]\n    基于 torch 的维度自动扩展，anchor_points 方便锚点位置转换的计算\n    \"\"\"\n    x1y1, x2y2 = bbox.chunk(2, -1)\n\t# 最后一个维度 [x1, y1, x2, y2]，分成 2 份，变成 [x1, y1] [x2,y2]。前面的维度不变。\n    # 先计算左、上、右、下的偏移量，然后将其取值范围压缩(clamp_) 到 [0, reg_max - 0.01] 之间\n    return torch.cat((anchor_points - x1y1, x2y2 - anchor_points), -1).clamp_(0, reg_max - 0.01)  # dist (lt, rb)\n\n\ndef dist2rbox(pred_dist, pred_angle, anchor_points, dim=-1):\n    \"\"\"\n    Decode predicted rotated bounding box coordinates from anchor points and distribution.\n\n    Args:\n        pred_dist (torch.Tensor): Predicted rotated distance, shape (bs, h*w, 4).\n        pred_angle (torch.Tensor): Predicted angle, shape (bs, h*w, 1).\n        anchor_points (torch.Tensor): Anchor points, shape (h*w, 2).\n        dim (int, optional): Dimension along which to split. Defaults to -1.\n\n    Returns:\n        (torch.Tensor): Predicted rotated bounding boxes, shape (bs, h*w, 4).\n    \"\"\"\n    lt, rb = pred_dist.split(2, dim=dim)\n    cos, sin = torch.cos(pred_angle), torch.sin(pred_angle)\n    # (bs, h*w, 1)\n    xf, yf = ((rb - lt) / 2).split(1, dim=dim)\n    x, y = xf * cos - yf * sin, xf * sin + yf * cos\n    xy = torch.cat([x, y], dim=dim) + anchor_points\n    return torch.cat([xy, lt + rb], dim=dim)\n"
        },
        {
          "label": "v8DetectionLoss",
          "language": "python",
          "value": "class BboxLoss(nn.Module):\n    \"\"\"Criterion class for computing training losses during training.\"\"\"\n\n    def __init__(self, reg_max, use_dfl=False):\n        \"\"\"Initialize the BboxLoss module with regularization maximum and DFL settings.\"\"\"\n        super().__init__()\n        self.reg_max = reg_max\n        self.use_dfl = use_dfl\n\n    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n        \"\"\"IoU loss.\"\"\"\n        weight = target_scores.sum(-1)[fg_mask].unsqueeze(-1)\n        iou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\n        loss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n\n        # DFL loss\n        if self.use_dfl:\n            target_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)\n            loss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight\n            loss_dfl = loss_dfl.sum() / target_scores_sum\n        else:\n            loss_dfl = torch.tensor(0.0).to(pred_dist.device)\n\n        return loss_iou, loss_dfl\n\n    @staticmethod\n    def _df_loss(pred_dist, target):\n        \"\"\"\n        Return sum of left and right DFL losses.\n\n        Distribution Focal Loss (DFL) proposed in Generalized Focal Loss\n        https://ieeexplore.ieee.org/document/9792391\n        \n        Args:\n            pred_dist (Tensor): shape(bs * num_total_anchors * 4, self.reg_max + 1)\n            target (Tensor):    shape(bs * num_total_anchors * 4,)（待确认）\n        \"\"\"\n        tl = target.long()  # target left，对应 DFL 公式的 y_i\n        tr = tl + 1  # target right，对应 DFL 公式的 y_{i+1}\n        wl = tr - target  # weight left，公式第 1 项的权重，对应 DFL 公式的 y_{i+1} - y\n        wr = 1 - wl  # weight right，公式第 2 项的权重，对应 DFL 公式的 y - y_i\n        return (\n            F.cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape) * wl # log(P_i) * wl\n            + F.cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape) * wr # log(P_{i+1})) * wr\n        ).mean(-1, keepdim=True)\n\nclass v8DetectionLoss:\n    \"\"\"Criterion class for computing training losses.\"\"\"\n\n    def __init__(self, model):  # model must be de-paralleled\n        \"\"\"Initializes v8DetectionLoss with the model, defining model-related properties and BCE loss function.\"\"\"\n        device = next(model.parameters()).device  # get model device\n        h = model.args  # hyperparameters\n\n        m = model.model[-1]  # Detect() module\n        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n        self.hyp = h\n        self.stride = m.stride  # model strides\n        self.nc = m.nc  # number of classes\n        self.no = m.nc + m.reg_max * 4 # 上一个 Module 返回的通道数量\n        self.reg_max = m.reg_max\n        self.device = device\n\n        self.use_dfl = m.reg_max > 1\n\n        self.assigner = TaskAlignedAssigner(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0)\n        self.bbox_loss = BboxLoss(m.reg_max - 1, use_dfl=self.use_dfl).to(device)\n        # 生成 0 到 reg_max-1（reg_max 取值 16）的整数列表，详见“如何表示偏移量”章节\n        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n\n    def preprocess(self, targets, batch_size, scale_tensor):\n        \"\"\"Preprocesses the target counts and matches with the input batch size to output a tensor.\"\"\"\n        if targets.shape[0] == 0:\n            out = torch.zeros(batch_size, 0, 5, device=self.device)\n        else:\n            i = targets[:, 0]  # image index\n            _, counts = i.unique(return_counts=True)\n            counts = counts.to(dtype=torch.int32)\n            out = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n            for j in range(batch_size):\n                matches = i == j\n                n = matches.sum()\n                if n:\n                    out[j, :n] = targets[matches, 1:]\n            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n        return out\n\n    def bbox_decode(self, anchor_points, pred_dist):\n        \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n        if self.use_dfl: # 不使用偏移量的绝对值，而是用一般分布表示。\n            b, a, c = pred_dist.shape  # batch, anchors, channels\n            # 对最后一维（reg_max）做 softmax 转换成映射表 [0，15] 中每个整数的概率，再乘以映射表，得到均值，作为该锚点的某个偏移量的预估值。\n            # pred_dist 变为 shape（batch, anchors, 4)\n            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n        return dist2bbox(pred_dist, anchor_points, xywh=False)\n\n    def __call__(self, preds, batch):\n        \"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\n        batch 为训练批次 dict（待确认），其中包括 GT 的 batch_idx, cls，bboxes 等属性\"\"\"\n        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n\n        # # Detect 推理时返回(y,x)，训练时返回 x，故这里根据返回类型提取特征。\n        # feats 为模型预测值，对于 Detect 任务，head.py 中 Detect 的输出为(x0,x1,x2)，x[0]的shape(N, reg_max*4+nc , H, W)\n        feats = preds[1] if isinstance(preds, tuple) else preds \n\n        # pred_distri 的形状为（N, reg_max*4, 锚点数为3个h*w的和）\n        # pred_scores 的形状为（N, nc, 锚点数为3个h*w的和）\n        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n            (self.reg_max * 4, self.nc), 1\n        )\n\n        # nn.Conv2d的维度是（N, C, H, W），为了后续方便基于锚点处理，这里维度顺序改变：\n        # pred_scores 的形状变为（N, 锚点数为3个h*w的和，nc）\n        # pred_distri 的形状变为（N, 锚点数为3个h*w的和，reg_max*4）\n        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n\n        dtype = pred_scores.dtype\n        batch_size = pred_scores.shape[0]\n        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n\n        # Targets\n        targets = torch.cat((batch[\"batch_idx\"].view(-1, 1), batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1)\n        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n\n        # Pboxes\n        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n\n        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n            pred_scores.detach().sigmoid(),\n            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n            anchor_points * stride_tensor,\n            gt_labels,\n            gt_bboxes,\n            mask_gt,\n        )\n\n        target_scores_sum = max(target_scores.sum(), 1)\n\n        # Cls loss\n        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n\n        # Bbox loss\n        if fg_mask.sum():\n            target_bboxes /= stride_tensor\n            loss[0], loss[2] = self.bbox_loss(\n                pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask\n            )\n\n        loss[0] *= self.hyp.box  # box gain\n        loss[1] *= self.hyp.cls  # cls gain\n        loss[2] *= self.hyp.dfl  # dfl gain\n\n        return loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)"
        }
      ],
      "id": "jpwj_tIA",
      "createdAt": 1731240053320,
      "updatedAt": 1731240702040
    }
  ],
  "tags": [
    {
      "name": "normal usage",
      "id": "7xSxQ3Xg",
      "createdAt": 1705650814819,
      "updatedAt": 1705650814819
    },
    {
      "name": "object array",
      "id": "7pTrIPxa",
      "createdAt": 1729698459359,
      "updatedAt": 1729698459359
    },
    {
      "name": "unique index",
      "id": "DRCnm5ma",
      "createdAt": 1730010904478,
      "updatedAt": 1730010904478
    }
  ]
}